<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Testing Forge (Stable Diffusion Flux) on an Old Nvidia 1080 Ti PC Running Linux Mint Wilma (Ubuntu…</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Testing Forge (Stable Diffusion Flux) on an Old Nvidia 1080 Ti PC Running Linux Mint Wilma (Ubuntu…</h1>
</header>
<section data-field="subtitle" class="p-summary">
Flux models and SDXL models are both important innovations in AI-generated image generation, particularly within the Stable Diffusion…
</section>
<section data-field="body" class="e-content">
<section name="a68a" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="59d1" id="59d1" class="graf graf--h3 graf--leading graf--title">Testing Forge (Stable Diffusion Flux) on an Old Nvidia 1080 Ti PC Running Linux Mint Wilma (Ubuntu 24.04 LTS) with Python 3.12, Python 3.11, Python3.10 and Xformers</h3><p name="ce2a" id="ce2a" class="graf graf--p graf-after--h3">Flux models and SDXL models are both important innovations in AI-generated image generation, particularly within the <strong class="markup--strong markup--p-strong">Stable Diffusion</strong> framework. Their differences and advantages primarily relate to the models’ architecture, efficiency, and intended use cases.</p><p name="aac5" id="aac5" class="graf graf--p graf-after--p graf--trailing">Take a closer look by reading this article <a href="https://medium.com/@promptingpixels/a-stable-diffusion-users-guide-to-understanding-flux-1-fee1e77c28a1" data-href="https://medium.com/@promptingpixels/a-stable-diffusion-users-guide-to-understanding-flux-1-fee1e77c28a1" class="markup--anchor markup--p-anchor" target="_blank">this article instead A Stable Diffusion User’s Guide to Understanding Flux.1</a></p></div></div></section><section name="ce04" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="aabd" id="aabd" class="graf graf--p graf--leading">If you’re not willing to wait for the Automatic1111 Flux update, you can use Forge, which is quite similar. I wanted to test whether I could run Forge on Linux Mint Wilma, using my old test PC with an Nvidia 1080 Ti, and to see if it’s true that Flux is faster than SDXL. This test was primarily a learning experience, and I’ve marked key lessons learned at the end of each topic.</p><p name="bb15" id="bb15" class="graf graf--p graf-after--p">It seems that Forge, compared to Automatic1111, is slower, even with SDXL models, despite being able to use Xformers with Forge to boost performance.</p><p name="603d" id="603d" class="graf graf--p graf-after--p">Below are the tests I conducted, along with the steps I followed to set up a virtual environment, clone Forge from GitHub, and get it working first with the unsupported Python 3.12, then with Python 3.11, and finally with the supported Python 3.10. I also tested it with Xformers, which, unfortunately, did not improve performance</p><p name="e89e" id="e89e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Note:</strong> If you have Python 3.10 or Python3.11, you can create a virtual environment using the command <code class="markup--code markup--p-code">python3.10 -m venv venv</code> and skip Python 3.12 section below.</p><h3 name="a51a" id="a51a" class="graf graf--h3 graf-after--p">Python 3.12</h3><p name="db01" id="db01" class="graf graf--p graf-after--h3">Let’ make virtual environment to /opt/forge:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="1479" id="1479" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">sudo <span class="hljs-built_in">mkdir</span> /opt/forge<br />sudo <span class="hljs-built_in">chown</span> yourname:yourgroup /opt/forge<br /><span class="hljs-built_in">cd</span> /opt/forge<br />python3 -m venv venv<br /><span class="hljs-built_in">source</span> venv/bin/activate<br /><br />git <span class="hljs-built_in">clone</span> https://github.com/lllyasviel/stable-diffusion-webui-forge.git<br /><span class="hljs-built_in">cd</span> stable-diffusion-forge<br />./webui.sh</span></pre><p name="5189" id="5189" class="graf graf--p graf-after--pre">Due to fact Ubuntu 24.04LTS and Mint Wilma comes with Python3.12, we see this note:</p><blockquote name="1d14" id="1d14" class="graf graf--blockquote graf-after--p">INCOMPATIBLE PYTHON VERSION</blockquote><blockquote name="9c0b" id="9c0b" class="graf graf--blockquote graf-after--blockquote">This program is tested with 3.10.6 Python, but you have 3.12.3.<br>If you encounter an error with “RuntimeError: Couldn’t install torch.” message,<br>or any other error regarding unsuccessful package (library) installation,<br>please downgrade (or upgrade) to the latest version of 3.10 Python<br>and delete current Python and “venv” folder in WebUI’s directory.</blockquote><p name="fd12" id="fd12" class="graf graf--p graf-after--blockquote">and as expected, install with Python 3.12 ended to error:</p><blockquote name="4b94" id="4b94" class="graf graf--blockquote graf-after--p">Building wheel for Pillow (pyproject.toml): finished with status ‘error’</blockquote><blockquote name="03fc" id="03fc" class="graf graf--blockquote graf-after--blockquote">ERROR: Could not build wheels for Pillow, which is required to install pyproject.toml-based projects</blockquote><p name="ed2f" id="ed2f" class="graf graf--p graf-after--blockquote">Let’s try to fix this problem by updating Pillow:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="csharp" name="144a" id="144a" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">sudo apt-<span class="hljs-keyword">get</span> install libjpeg-dev zlib1g-dev</span></pre><p name="85e7" id="85e7" class="graf graf--p graf-after--pre">Pillow update ended now to error:</p><blockquote name="202e" id="202e" class="graf graf--blockquote graf-after--p">RuntimeError: Failed to import diffusers.pipelines.pipeline_utils because of the following error (look up to see its traceback): Failed to import diffusers.models.autoencoders.autoencoder_kl because of the following error (look up to see its traceback): No module named ‘distutils’”</blockquote><p name="163a" id="163a" class="graf graf--p graf-after--blockquote">Main Issues:</p><ol class="postList"><li name="60ae" id="60ae" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Invalid escape sequence </strong><code class="markup--code markup--li-code"><strong class="markup--strong markup--li-strong">\(</strong></code>: The warning mentions an invalid escape sequence in the file <code class="markup--code markup--li-code">/opt/forge/stable-diffusion-webui-forge/modules/prompt_parser.py:387</code>. This could likely be caused by improperly escaped characters in the Python string literals. To fix this, you can replace <code class="markup--code markup--li-code">\(</code> with <code class="markup--code markup--li-code">\\(</code>, or use raw strings by prefixing the string with <code class="markup--code markup--li-code">r&quot;...&quot;</code> to avoid Python interpreting escape sequences.</li><li name="7c34" id="7c34" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code"><strong class="markup--strong markup--li-strong">ModuleNotFoundError: No module named &#39;distutils&#39;</strong></code>:</li></ol><ul class="postList"><li name="b718" id="b718" class="graf graf--li graf-after--li">This is the main issue that’s causing the failure. The <code class="markup--code markup--li-code"><strong class="markup--strong markup--li-strong">distutils</strong></code> module is missing in your Python environment. It is a legacy module for utility functions like <code class="markup--code markup--li-code">strtobool</code> in <code class="markup--code markup--li-code">distutils.util</code>.</li><li name="ced5" id="ced5" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Python 3.12</strong> <em class="markup--em markup--li-em">no longer includes </em><code class="markup--code markup--li-code"><em class="markup--em markup--li-em">distutils</em></code><em class="markup--em markup--li-em"> by default as part of the standard library, as it was deprecated and eventually removed.</em></li></ul><p name="4f9d" id="4f9d" class="graf graf--p graf-after--li">Ok, <strong class="markup--strong markup--p-strong">distutils is not supported</strong>, let’s fix distutils-problem by updating setuptools to proper version:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="css" name="afca" id="afca" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">python -m pip install <span class="hljs-attr">--upgrade</span> setuptools</span></pre><p name="a4c5" id="a4c5" class="graf graf--p graf-after--pre">Then start again with <strong class="markup--strong markup--p-strong">./webui.sh</strong> and now Forge seems to load normally.</p><p name="4133" id="4133" class="graf graf--p graf-after--p">If you face insightface issue, do following:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="typescript" name="2e83" id="2e83" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">pip install protobuf==<span class="hljs-number">3.20</span><span class="hljs-number">.0</span><br />pip install protobuf==<span class="hljs-number">4.25</span><span class="hljs-number">.3</span><br />pip install insightface</span></pre><blockquote name="6d31" id="6d31" class="graf graf--blockquote graf-after--pre">pytorch version: 2.3.1+cu121<br>Device: cuda:0 NVIDIA GeForce GTX 1080 Ti : native<br>…<br>[GPU Setting] You will use 90.82% GPU memory (10133.00 MB) to load weights, and use 9.18% GPU memory (1024.00 MB) to do matrix computation.<br>You do not have any model!<br>Model selected: {‘checkpoint_info’: None, ‘additional_modules’: [], ‘unet_storage_dtype’: None}<br>Using online LoRAs in FP16: False</blockquote><p name="24b3" id="24b3" class="graf graf--p graf-after--blockquote">So, now the Forge backend and frontend seems to load correctly.</p><p name="fffe" id="fffe" class="graf graf--p graf-after--p">Let’s see can we create some picture with Flux. For that we need to download a Flux-model. Download from <a href="https://huggingface.co/lllyasviel/flux1-dev-bnb-nf4/tree/main" data-href="https://huggingface.co/lllyasviel/flux1-dev-bnb-nf4/tree/main" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">HuggingFace link</a> latest model file, today it is <a href="https://huggingface.co/lllyasviel/flux1-dev-bnb-nf4/blob/main/flux1-dev-bnb-nf4-v2.safetensors" data-href="https://huggingface.co/lllyasviel/flux1-dev-bnb-nf4/blob/main/flux1-dev-bnb-nf4-v2.safetensors" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">flux1-dev-bnb-nf4-v2.safetensors</a> (12GB) and save it to /opt/forge/stable-diffusion-webui-forge/models/Stable-diffusion folder.</p><p name="3260" id="3260" class="graf graf--p graf-after--p">Note! With Nvidia 1080Ti NF4 is NOT option, you have to download <a href="https://huggingface.co/lllyasviel/flux1_dev/blob/main/flux1-dev-fp8.safetensors" data-href="https://huggingface.co/lllyasviel/flux1_dev/blob/main/flux1-dev-fp8.safetensors" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://huggingface.co/lllyasviel/flux1_dev/blob/main/flux1-dev-fp8.safetensors</a> instead! (this I did not know yesterday).</p><p name="c0f2" id="c0f2" class="graf graf--p graf-after--p">Tip: NF4 refers to 4-bit non-floating point precision, BNB stands for BitsandBytes, a low-bit accelerator model, and GGUF is a binary format optimized for quick loading and saving of models.</p><p name="4cb8" id="4cb8" class="graf graf--p graf-after--p">First generation was extremely slow. I used default startup parameters, Euler (Simple), 20 steps, 3,5 CFG and size 896x1152. Generation took 12:15 minutes, but no errors:</p><blockquote name="3a2d" id="3a2d" class="graf graf--blockquote graf-after--p">All loaded to GPU.<br><br>Total progress: 100%|████████████████████████████████████████████████████████████████████████████████████| 20/20 [12:15&lt;00:00, 37.11s/it]</blockquote><figure name="cd6a" id="cd6a" class="graf graf--figure graf-after--blockquote"><img class="graf-image" data-image-id="1*r49kHOayurzYhuWyUipY3A.png" data-width="896" data-height="1152" src="https://cdn-images-1.medium.com/max/800/1*r49kHOayurzYhuWyUipY3A.png"><figcaption class="imageCaption">First picture made with Forge Flux and Python3.12</figcaption></figure><p name="7b2f" id="7b2f" class="graf graf--p graf-after--figure">So, Forge works with Python3.12, but is speed issue related to Python version?</p><p name="355a" id="355a" class="graf graf--p graf-after--p">Lessons learned:</p><ol class="postList"><li name="93d8" id="93d8" class="graf graf--li graf-after--p">Python3.12 works, but perhaps it is easier to use older versions.</li><li name="e545" id="e545" class="graf graf--li graf-after--li graf--trailing">Nvidia 1080 Ti does not support NF4, so, in fact I shall use <a href="https://huggingface.co/lllyasviel/flux1_dev/blob/main/flux1-dev-fp8.safetensors" data-href="https://huggingface.co/lllyasviel/flux1_dev/blob/main/flux1-dev-fp8.safetensors" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">https://huggingface.co/lllyasviel/flux1_dev/blob/main/flux1-dev-fp8.safetensors</a> instead (17 gigs)</li></ol></div></div></section><section name="eb27" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="f053" id="f053" class="graf graf--h3 graf--leading">Python 3.11 and xformers</h3><p name="5652" id="5652" class="graf graf--p graf-after--h3">Let’s delete venv and use Python3.11, which works for Automatic1111 just fine and should work for Forge too:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="686b" id="686b" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-built_in">cd</span> ..<br />deactivate<br /><span class="hljs-built_in">rm</span> -rf venv<br />python3.11 -m venv venv<br /><span class="hljs-built_in">source</span> venv/bin/activate<br />./webui.sh</span></pre><p name="499f" id="499f" class="graf graf--p graf-after--pre">Installation ended without errors.</p><p name="bd30" id="bd30" class="graf graf--p graf-after--p">Backend started without Python3.10 requirement notification, presuming Python3.11 is ok as predicted.</p><p name="d7bc" id="d7bc" class="graf graf--p graf-after--p">Let’s create something with prompt: “a portrait of emma stone dressed as a belly dancer, arabian night, high quality, fully detailed, 4 k, in focus sharp face with fine details, realistic hand details and anatomy, inspired by belly dancer shakira on youtube, by artgerm and greg rutkowski and alphonse mucha, masterpiece, stunning, artstation”</p><p name="af1d" id="af1d" class="graf graf--p graf-after--p">Using default startup parameters (Euler simpe, 20 steps):</p><figure name="80c1" id="80c1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ll7bZIObFPC-NvMiBQIRIg.png" data-width="1515" data-height="743" src="https://cdn-images-1.medium.com/max/800/1*Ll7bZIObFPC-NvMiBQIRIg.png"><figcaption class="imageCaption">Forge with Python3.11</figcaption></figure><p name="10b8" id="10b8" class="graf graf--p graf-after--figure">Generation was slow, 12:08 minutes and most likely not related to Python-version.</p><blockquote name="a467" id="a467" class="graf graf--blockquote graf-after--p">All loaded to GPU.<br><br>Total progress: 100%|████████████████████████████████████████████████████████████████████████████████████| 20/20 [12:08&lt;00:00, 36.44s/it]</blockquote><figure name="9701" id="9701" class="graf graf--figure graf-after--blockquote"><img class="graf-image" data-image-id="1*qdFjXBjved8eI1G4B-2IKA.png" data-width="896" data-height="1152" src="https://cdn-images-1.medium.com/max/800/1*qdFjXBjved8eI1G4B-2IKA.png"><figcaption class="imageCaption">Sedond picture created with Forge Flux and Python3.11</figcaption></figure><p name="4b68" id="4b68" class="graf graf--p graf-after--figure">Perhaps installing xformers speeds up things? Let’s try that.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="6357" id="6357" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-built_in">source</span> venv/bin/activate<br />pip install xformers</span></pre><p name="284e" id="284e" class="graf graf--p graf-after--pre">Installation uninstalled triton, cuda, torch, complained about too new protobuf, and then tried to install with error:</p><blockquote name="a097" id="a097" class="graf graf--blockquote graf-after--p">ERROR: pip’s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. open-clip-torch 2.20.0 requires protobuf&lt;4, but you have protobuf 4.25.5 which is incompatible.<br>torchvision 0.18.1+cu121 requires torch==2.3.1, but you have torch 2.4.1 which is incompatible.</blockquote><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="sql" name="a664" id="a664" class="graf graf--pre graf-after--blockquote graf--preV2"><span class="pre--content">pip <span class="hljs-keyword">check</span></span></pre><blockquote name="7ece" id="7ece" class="graf graf--blockquote graf-after--pre">xformers 0.0.28.post1 has requirement torch==2.4.1, but you have torch 2.3.1.</blockquote><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="typescript" name="ffbf" id="ffbf" class="graf graf--pre graf-after--blockquote graf--preV2"><span class="pre--content">pip install torch==<span class="hljs-number">2.4</span><span class="hljs-number">.1</span></span></pre><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="sql" name="0ea9" id="0ea9" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">pip <span class="hljs-keyword">check</span></span></pre><blockquote name="2328" id="2328" class="graf graf--blockquote graf-after--pre">Now torchvision 0.18.1+cu121 has requirement torch==2.3.1, but you have torch 2.4.1.</blockquote><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="typescript" name="41b2" id="41b2" class="graf graf--pre graf-after--blockquote graf--preV2"><span class="pre--content">pip install torch==<span class="hljs-number">2.3</span><span class="hljs-number">.1</span></span></pre><p name="7f4e" id="7f4e" class="graf graf--p graf-after--pre">And now I end up to xformers error :)</p><p name="a8d7" id="a8d7" class="graf graf--p graf-after--p">There’s a circular dependency conflict between the versions of <code class="markup--code markup--p-code">torch</code>, <code class="markup--code markup--p-code">torchvision</code>, and <code class="markup--code markup--p-code">xformers</code>. Let’s see this further:</p><ol class="postList"><li name="3fc4" id="3fc4" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code"><strong class="markup--strong markup--li-strong">torchvision</strong></code> requires <code class="markup--code markup--li-code">torch==2.3.1</code>.</li><li name="836b" id="836b" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code"><strong class="markup--strong markup--li-strong">xformers</strong></code> requires <code class="markup--code markup--li-code">torch==2.4.1</code>.</li><li name="6067" id="6067" class="graf graf--li graf-after--li">Version of <code class="markup--code markup--li-code">torch</code> installed that may not match these requirements.</li></ol><p name="cd35" id="cd35" class="graf graf--p graf-after--li">Fine, let’s get rid off both:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="typescript" name="b6d6" id="b6d6" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">pip uninstall torch torchvision xformers</span></pre><p name="8879" id="8879" class="graf graf--p graf-after--pre">and then start Forge with <strong class="markup--strong markup--p-strong">./webui.sh</strong>, which installs required packages again and then again try to install xformers.</p><p name="33c5" id="33c5" class="graf graf--p graf-after--p">Pip showed a few notes about imcompatiblies, but successfully installed nvidia-cudnn-cu12–9.1.0.70 torch-2.4.1 triton-3.0.0 xformers-0.0.28.post1.</p><p name="7526" id="7526" class="graf graf--p graf-after--p">Let’s see what happens if we execute <strong class="markup--strong markup--p-strong">./webui.sh</strong> again.</p><p name="00a4" id="00a4" class="graf graf--p graf-after--p">Now ended to crash:</p><blockquote name="8b84" id="8b84" class="graf graf--blockquote graf-after--p">WARNING:xformers:WARNING[XFORMERS]: xFormers can’t load C++/CUDA extensions. xFormers was built for:<br> PyTorch 2.4.1+cu121 with CUDA 1201 (you have 2.3.1+cu121)<br> Python 3.11.10 (you have 3.11.10)<br> Please reinstall xformers (see <a href="https://github.com/facebookresearch/xformers#installing-xformers" data-href="https://github.com/facebookresearch/xformers#installing-xformers" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener" target="_blank">https://github.com/facebookresearch/xformers#installing-xformers</a>)<br> Memory-efficient attention, SwiGLU, sparse and more won’t be available.<br> Set XFORMERS_MORE_DETAILS=1 for more details</blockquote><blockquote name="7063" id="7063" class="graf graf--blockquote graf-after--blockquote">RuntimeError: Failed to import diffusers.pipelines.pipeline_utils because of the following error (look up to see its traceback):<br>Failed to import diffusers.models.autoencoders.autoencoder_kl because of the following error (look up to see its traceback):<br>module ‘torch.library’ has no attribute ‘custom_op’</blockquote><p name="23d3" id="23d3" class="graf graf--p graf-after--blockquote">Fine, xformers is problematic, let’s pip uninstall xformers and install cu124 version:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="perl" name="ff13" id="ff13" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">pip3 install -U xformers --<span class="hljs-keyword">index</span>-url https:<span class="hljs-regexp">//d</span>ownload.pytorch.org/whl/cu124</span></pre><p name="3340" id="3340" class="graf graf--p graf-after--pre">Now results looks better:</p><blockquote name="ccf3" id="ccf3" class="graf graf--blockquote graf-after--p">Successfully installed nvidia-cublas-cu12–12.4.2.65 nvidia-cuda-cupti-cu12–12.4.99 nvidia-cuda-nvrtc-cu12–12.4.99 nvidia-cuda-runtime-cu12–12.4.99 nvidia-cudnn-cu12–9.1.0.70 nvidia-cufft-cu12–11.2.0.44 nvidia-curand-cu12–10.3.5.119 nvidia-cusolver-cu12–11.6.0.99 nvidia-cusparse-cu12–12.3.0.142 nvidia-nvjitlink-cu12–12.4.99 nvidia-nvtx-cu12–12.4.99 torch-2.4.1+cu124 triton-3.0.0 xformers-0.0.28.post1</blockquote><p name="b845" id="b845" class="graf graf--p graf-after--blockquote">Let’s see what executing <strong class="markup--strong markup--p-strong">./webui.sh</strong> says:</p><blockquote name="282b" id="282b" class="graf graf--blockquote graf-after--p">pytorch version: 2.4.1+cu124<br>xformers version: 0.0.28.post1<br>Set vram state to: NORMAL_VRAM<br>Device: cuda:0 NVIDIA GeForce GTX 1080 Ti : native<br>VAE dtype preferences: [torch.float32] -&gt; torch.float32<br>Using xformers cross attention<br>Using xformers attention for VAE</blockquote><blockquote name="6aa2" id="6aa2" class="graf graf--blockquote graf-after--blockquote">Error: AttributeError: partially initialized module ‘torchvision’ has no attribute ‘extension’ (most likely due to a circular import)</blockquote><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="sql" name="deed" id="deed" class="graf graf--pre graf-after--blockquote graf--preV2"><span class="pre--content">pip <span class="hljs-keyword">check</span></span></pre><blockquote name="e595" id="e595" class="graf graf--blockquote graf-after--pre">mediapipe 0.10.15 has requirement protobuf&lt;5,&gt;=4.25.3, but you have protobuf 3.20.0.<br>onnx 1.17.0 has requirement protobuf&gt;=3.20.2, but you have protobuf 3.20.0.<br>torchvision 0.18.1+cu121 has requirement torch==2.3.1, but you have torch 2.4.1+cu124.</blockquote><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="lua" name="16f2" id="16f2" class="graf graf--pre graf-after--blockquote graf--preV2"><span class="pre--content">pip install mediapipe <span class="hljs-comment">--upgrade</span><br />pip install protobuf==<span class="hljs-number">3.20</span><span class="hljs-number">.2</span><br />pip install torchvision==<span class="hljs-number">0.19</span><span class="hljs-number">.0</span></span></pre><blockquote name="6c04" id="6c04" class="graf graf--blockquote graf-after--pre">Successfully installed nvidia-cublas-cu12–12.1.3.1 nvidia-cuda-cupti-cu12–12.1.105 nvidia-cuda-nvrtc-cu12–12.1.105 nvidia-cuda-runtime-cu12–12.1.105 nvidia-cufft-cu12–11.0.2.54 nvidia-curand-cu12–10.3.2.106 nvidia-cusolver-cu12–11.4.5.107 nvidia-cusparse-cu12–12.1.0.106 nvidia-nvtx-cu12–12.1.105 torch-2.4.0 torchvision-0.19.0</blockquote><p name="a48b" id="a48b" class="graf graf--p graf-after--blockquote">Now pip check complains about mediapipe and xformers:</p><blockquote name="bc99" id="bc99" class="graf graf--blockquote graf-after--p">mediapipe 0.10.15 has requirement protobuf&lt;5,&gt;=4.25.3, but you have protobuf 3.20.2.<br>xformers 0.0.28.post1 has requirement torch==2.4.1, but you have torch 2.4.0.</blockquote><p name="940f" id="940f" class="graf graf--p graf-after--blockquote">But Forge starts now ok with xformers:</p><blockquote name="0fdf" id="0fdf" class="graf graf--blockquote graf-after--p">Python 3.11.10 (main, Sep 7 2024, 18:35:41) [GCC 13.2.0]<br>Version: f2.0.1v1.10.1-previous-569-g6dc71b7e<br>pytorch version: 2.4.0+cu121<br>xformers version: 0.0.28.post1<br>Device: cuda:0 NVIDIA GeForce GTX 1080 Ti : native<br><strong class="markup--strong markup--blockquote-strong">Using xformers cross attention<br>Using xformers attention for VAE</strong></blockquote><p name="c1ed" id="c1ed" class="graf graf--p graf-after--blockquote">Let’s test again with default startup parameters (Eurler simple, 20 steps):</p><p name="33b7" id="33b7" class="graf graf--p graf-after--p">Prompt: Medieval Harley Quinn and Joker, portrait, playful, fantasy, medieval, beautiful face, vivid colrs, elegant, concept art, sharp focus, digital art, Hyper-realistic, 4K, Unreal Engine, Highly Detailed, HD, Dramatic Lighting by Brom, trending on Artstation.</p><p name="7ef6" id="7ef6" class="graf graf--p graf-after--p">Generation with xformers was still slow.</p><blockquote name="453b" id="453b" class="graf graf--blockquote graf-after--p">All loaded to GPU.<br>…<br>Total progress: 100%|████████████████████████████████████████████████████████████████████████████████████| 20/20 [12:03&lt;00:00, 36.41s/it</blockquote><figure name="9e69" id="9e69" class="graf graf--figure graf-after--blockquote"><img class="graf-image" data-image-id="1*937nkUlqgG2FhoUbtxFAcQ.png" data-width="896" data-height="1152" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*937nkUlqgG2FhoUbtxFAcQ.png"><figcaption class="imageCaption">Picture numer 3 with Forge, Flux, Python 3.11 and xformers.</figcaption></figure><p name="c1de" id="c1de" class="graf graf--p graf-after--figure">Result looks … good, but generation was slow. Perhaps we really need Python3.10 and slowness is related to Python-version (xformers).</p><p name="d9de" id="d9de" class="graf graf--p graf-after--p">Lessons learned:</p><ol class="postList"><li name="31ce" id="31ce" class="graf graf--li graf-after--p graf--trailing">Python3.11 migh work, but Xformers remains problmatic</li></ol></div></div></section><section name="9914" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="759f" id="759f" class="graf graf--h3 graf--leading">Python 3.10</h3><p name="c340" id="c340" class="graf graf--p graf-after--h3">Let’s delete venv again and install python3.10 and then create venv:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="4325" id="4325" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-built_in">rm</span> -rf venv<br />sudo apt install python3.10 python3.10-venv python3.10-dev<br />python3.10 -m venv venv<br /><span class="hljs-built_in">source</span> venv/bin/activate<br />pip install --upgrade pip<br />./webui.sh</span></pre><p name="a1fc" id="a1fc" class="graf graf--p graf-after--pre">Installation went smoothly, no errors.</p><p name="59fc" id="59fc" class="graf graf--p graf-after--p">Let’s install xformers, try version cu124 first because my nvidia-smi shows NVIDIA-SMI 550.107.02 Driver Version: 550.107.02 CUDA Version: 12.4. You can check which xformers fits to you:</p><div name="3b75" id="3b75" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/facebookresearch/xformers#installing-xformers" data-href="https://github.com/facebookresearch/xformers#installing-xformers" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/facebookresearch/xformers#installing-xformers"><strong class="markup--strong markup--mixtapeEmbed-strong">GitHub - facebookresearch/xformers: Hackable and optimized Transformers building blocks, supporting…</strong><br><em class="markup--em markup--mixtapeEmbed-em">Hackable and optimized Transformers building blocks, supporting a composable construction. - facebookresearch/xformers</em>github.com</a><a href="https://github.com/facebookresearch/xformers#installing-xformers" class="js-mixtapeImage mixtapeImage mixtapeImage--empty u-ignoreBlock" data-media-id="c204d277b898b802c9751b678b3d3dc9"></a></div><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="perl" name="9a00" id="9a00" class="graf graf--pre graf-after--mixtapeEmbed graf--preV2"><span class="pre--content">pip3 install -U xformers --<span class="hljs-keyword">index</span>-url https:<span class="hljs-regexp">//d</span>ownload.pytorch.org/whl/cu124</span></pre><p name="db4c" id="db4c" class="graf graf--p graf-after--pre">Crash to AttributeError: partially initialized module ‘torchvision’ has no attribute ‘extension’ (most likely due to a circular import). Perhaps version 124 of Pytorch is too new, so, uninstall xformers and try older:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="perl" name="79f4" id="79f4" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">pip uninstall xformers<br />pip3 install -U xformers --<span class="hljs-keyword">index</span>-url https:<span class="hljs-regexp">//d</span>ownload.pytorch.org/whl/cu121<br /><br /></span></pre><p name="ecda" id="ecda" class="graf graf--p graf-after--pre">Same thing, crash to AttributeError: partially initialized module ‘torchvision’ has no attribute ‘extension’ (most likely due to a circular import). It seems that pytorch is now too new and lazy I am, it is simplier to delete venv and create again and install required packages:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="d924" id="d924" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-built_in">rm</span> -rf venv<br />python3.10 -m venv venv<br /><span class="hljs-built_in">source</span> venv/bin/activate<br />pip install --upgrade pip<br />./webui.sh</span></pre><p name="89f4" id="89f4" class="graf graf--p graf-after--pre">pytorch version:<strong class="markup--strong markup--p-strong"> 2.3.1+cu121</strong> so pip3 install -U xformers — index-url <a href="https://download.pytorch.org/whl/cu121" data-href="https://download.pytorch.org/whl/cu121" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://download.pytorch.org/whl/cu121</a> should do it, but it seems to update too much.</p><blockquote name="d0c2" id="d0c2" class="graf graf--blockquote graf-after--p">Successfully installed nvidia-cudnn-cu12–9.1.0.70 torch-2.4.1+cu121 triton-3.0.0 xformers-0.0.28.post1</blockquote><p name="c1f8" id="c1f8" class="graf graf--p graf-after--blockquote">./webui.sh loads the backend ok:</p><blockquote name="fc77" id="fc77" class="graf graf--blockquote graf-after--p"><strong class="markup--strong markup--blockquote-strong">pytorch version: 2.4.1+cu121</strong><br>xformers version: 0.0.28.post1<br>Set vram state to: NORMAL_VRAM<br>Device: cuda:0 NVIDIA GeForce GTX 1080 Ti : native<br>VAE dtype preferences: [torch.float32] -&gt; torch.float32<br>Using xformers cross attention<br>Using xformers attention for VAE</blockquote><p name="eb97" id="eb97" class="graf graf--p graf-after--blockquote">So, test with defaults as before (Euler simple, 20 steps), same prompt:</p><p name="5dad" id="5dad" class="graf graf--p graf--startsWithDoubleQuote graf-after--p">“Medieval Harley Quinn and Joker, portrait, playful, fantasy, medieval, beautiful face, vivid colrs, elegant, concept art, sharp focus, digital art, Hyper-realistic, 4K, Unreal Engine, Highly Detailed, HD, Dramatic Lighting by Brom, trending on Artstation.”</p><figure name="0890" id="0890" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fRJucLx8N5yLv4UxBjB5uw.png" data-width="758" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*fRJucLx8N5yLv4UxBjB5uw.png"><figcaption class="imageCaption">GPU utilization is not 100%</figcaption></figure><p name="3ce5" id="3ce5" class="graf graf--p graf-after--figure">It is still slow, generation time 12:04.</p><figure name="3512" id="3512" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Vt-CYcFOW2p8AevGTC1kmA.png" data-width="896" data-height="1152" src="https://cdn-images-1.medium.com/max/800/1*Vt-CYcFOW2p8AevGTC1kmA.png"><figcaption class="imageCaption">Picture number 4 Forge on Python 3.10 and <strong class="markup--strong markup--figure-strong">pytorch version: 2.4.1+cu121 </strong>xformers version: .0.28.post1</figcaption></figure><p name="1626" id="1626" class="graf graf--p graf-after--figure">Lesson learned:</p><ol class="postList"><li name="6cf1" id="6cf1" class="graf graf--li graf-after--p">Pytorch versions working with Xformers do not improve speed at all.</li></ol><h3 name="70b2" id="70b2" class="graf graf--h3 graf-after--li">Forge with Python3.10 and xformers using Euler beta sampler</h3><p name="af21" id="af21" class="graf graf--p graf-after--h3">Just for fun, let’s try Eurler with Beta setting and reduce steps to 10. Should generation time be half compared to previous 20 steps and Euler simple?</p><p name="7d96" id="7d96" class="graf graf--p graf-after--p">Yes, generation time was about half, 05:50, 35.03s/it, but result is greepy :)</p><figure name="2cef" id="2cef" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*QThDeJi4mdKbkolNxwscpQ.png" data-width="896" data-height="1152" src="https://cdn-images-1.medium.com/max/800/1*QThDeJi4mdKbkolNxwscpQ.png"><figcaption class="imageCaption">Flux, Euler beta, 10 steps</figcaption></figure><figure name="9a93" id="9a93" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*GTPv7Fk30bvtKeHdwzxAog.png" data-width="1024" data-height="1024" src="https://cdn-images-1.medium.com/max/800/1*GTPv7Fk30bvtKeHdwzxAog.png"><figcaption class="imageCaption">Robot holding paper with handwritten text “Halloween 2024”. Inside frozen iceball, firemode, burning in flame, water is dripping from ice, flames are turning twisting smoke,air and earth elements. Golden ratio, insane details, masterpiece, 35mm photography, dslr, kodachrome, 8k, hdr, vivid and vibrant colors, reflections.<br> Steps: 20, Sampler: Euler, Schedule type: Simple, CFG scale: 1, Distilled CFG Scale: 1.5, Seed: 97913728, Size: 1024x1024, Model hash: bea01d51bd, Model: flux1-dev-bnb-nf4-v2, Version: f2.0.1v1.10.1-previous-569-g6dc71b7e. Time taken: 10 min. 42.7 sec.</figcaption></figure><p name="31a3" id="31a3" class="graf graf--p graf-after--figure">Same prompt with <a href="https://huggingface.co/ZhenyaYang/flux_1_dev_hyper_8steps_nf4/tree/main" data-href="https://huggingface.co/ZhenyaYang/flux_1_dev_hyper_8steps_nf4/tree/main" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">flux_1_dev_hyper_8steps_nf4.safetensors</a> (this should not work with Nvidia 1080 Ti, but it worked)</p><figure name="0e30" id="0e30" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Z-6dIr2PtCRzV7DHXR7JNQ.png" data-width="1024" data-height="1024" src="https://cdn-images-1.medium.com/max/800/1*Z-6dIr2PtCRzV7DHXR7JNQ.png"><figcaption class="imageCaption">Robot holding paper with handwritten text “Halloween 2024”. Inside frozen iceball, firemode, burning in flame, water is dripping from ice, flames are turning twisting smoke,air and earth elements. Golden ratio, insane details, masterpiece, 35mm photography, dslr, kodachrome, 8k, hdr, vivid and vibrant colors, reflections.<br> Steps: 20, Sampler: Euler, Schedule type: Beta, CFG scale: 1, Distilled CFG Scale: 7, Seed: 2365019855, Size: 1024x1024, Model hash: 7e8c93b83c, Model: flux_1_dev_hyper_8steps_nf4, Beta schedule alpha: 0.6, Beta schedule beta: 0.6, Version: f2.0.1v1.10.1-previous-569-g6dc71b7e. Time taken: 10 min. 40.5 sec.</figcaption></figure><figure name="8311" id="8311" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*ybSuvvBA9hAikZH5Dyi21A.png" data-width="896" data-height="1152" src="https://cdn-images-1.medium.com/max/800/1*ybSuvvBA9hAikZH5Dyi21A.png"><figcaption class="imageCaption">Robot (metallic creature) laughing and holding flag of Finland (blue cross, white). On top of Tuomiokirkko (Finland) roof. Golden ratio, insane details, masterpiece, 35mm photography, dslr, kodachrome, 8k, hdr, vivid and vibrant colors, reflections.<br> Steps: 10, Sampler: Euler, Schedule type: Beta, CFG scale: 1, Distilled CFG Scale: 3.5, Seed: 3837994734, Size: 896x1152, Model hash: 7e8c93b83c, Model: flux_1_dev_hyper_8steps_nf4, Beta schedule alpha: 0.6, Beta schedule beta: 0.6, Version: f2.0.1v1.10.1-previous-569-g6dc71b7e. Time taken: 6 min. 26.7 sec.</figcaption></figure><h3 name="4d64" id="4d64" class="graf graf--h3 graf-after--figure">Automatic1111 with Python3.11</h3><p name="9187" id="9187" class="graf graf--p graf-after--h3">Let’s compare speed to Automatic1111 SDXL with same prompt in the same computer, with xformers:</p><blockquote name="aa74" id="aa74" class="graf graf--blockquote graf-after--p">…</blockquote><blockquote name="31f9" id="31f9" class="graf graf--blockquote graf-after--blockquote">Applying attention optimization: xformers… done.<br>Model loaded in 52.7s (load weights from disk: 1.7s, create model: 0.5s, apply weights to model: 48.4s, apply half(): 0.1s, calculate empty prompt: 1.9s).<br>100%|█████████████████████████████████████████████| 7/7 [00:30&lt;00:00, 4.36s/it]<br>[Tiled VAE]: the input size is tiny and unnecessary to tile.26&lt;00:00, 4.17s/it]<br>Total progress: 100%|█████████████████████████████| 7/7 [00:29&lt;00:00, 4.23s/it]<br>Total progress: 100%|█████████████████████████████| 7/7 [00:29&lt;00:00, 4.17s/it]</blockquote><figure name="1371" id="1371" class="graf graf--figure graf-after--blockquote"><img class="graf-image" data-image-id="1*soLu1YGydRoCvErvy7TZzg.png" data-width="1024" data-height="1024" src="https://cdn-images-1.medium.com/max/800/1*soLu1YGydRoCvErvy7TZzg.png"><figcaption class="imageCaption"><code class="markup--code markup--figure-code">Automatic1111, Steps: 7, Sampler: DPM++ SDE, Schedule type: Karras, CFG scale: 2, Seed: 2328055557, Size: 1024x1024, Model hash: 4496b36d48, Model: dreamshaperXL_v21TurboDPMSDE, Version: v1.10.1, Source Identifier: Stable Diffusion web UI</code></figcaption></figure><p name="3a8b" id="3a8b" class="graf graf--p graf-after--figure">Generation time was 0:29 minutes and result look OK.</p><p name="692c" id="692c" class="graf graf--p graf-after--p">Lesson learned:</p><ol class="postList"><li name="1d4f" id="1d4f" class="graf graf--li graf-after--p">Fitting torch etc. to CUDA version 12.4 needs work, but perhaps there is no reason to update from default 12.1 (<strong class="markup--strong markup--li-strong">2.3.1+cu121) </strong>if it works in a similar way.</li><li name="99a8" id="99a8" class="graf graf--li graf-after--li">For some reason I do not understand Automatic1111 is faster than Forge.</li></ol><h3 name="6d8a" id="6d8a" class="graf graf--h3 graf-after--li">Forge with SDXL models</h3><p name="a6db" id="a6db" class="graf graf--p graf-after--h3">Remember, that if you run Forge and Automatic1111 in the same computer, you may either setup paths in settings, or use soft links like ln -s /opt/stable-diffusion-webui/models/Stable-diffusion/ /opt/forge/stable-diffusion-webui-forge/models/Stable-diffusion/ etc.</p><p name="1f10" id="1f10" class="graf graf--p graf-after--p">Previous picture generation time with Automatic1111 was 0:29. Let’s see speed if we use Forge and same dreamshaperXL model:</p><figure name="0655" id="0655" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pBZkjnnFEhcdzfcGDdFkOA.png" data-width="896" data-height="1152" src="https://cdn-images-1.medium.com/max/800/1*pBZkjnnFEhcdzfcGDdFkOA.png"><figcaption class="imageCaption">Medieval Harley Quinn and Joker, portrait, playful, fantasy, medieval, beautiful face, vivid colrs, elegant, concept art, sharp focus, digital art, Hyper-realistic, 4K, Unreal Engine, Highly Detailed, HD, Dramatic Lighting by Brom, trending on Artstation.<br> Steps: 8, Sampler: DPM++ SDE, Schedule type: Karras, CFG scale: 1.9, Seed: 4171555793, Size: 896x1152, Model hash: 3d0e279924, Model: dreamshaperXL_v21TurboDPMSDE, Version: f2.0.1v1.10.1-previous-569-g6dc71b7e</figcaption></figure><p name="65c4" id="65c4" class="graf graf--p graf-after--figure">Generation time was 00:53, not that fast as Automatic111, but fast enough.</p><figure name="5193" id="5193" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fkmMRgypxams_YQ34AcBxw.png" data-width="1024" data-height="1024" src="https://cdn-images-1.medium.com/max/800/1*fkmMRgypxams_YQ34AcBxw.png"><figcaption class="imageCaption">Robot holding paper with handwritten text “Halloween 2024”. Inside frozen iceball, firemode, burning in flame, water is dripping from ice, flames are turning twisting smoke,air and earth elements. Golden ratio, insane details, masterpiece, 35mm photography, dslr, kodachrome, 8k, hdr, vivid and vibrant colors, reflections.<br> Steps: 10, Sampler: DPM++ SDE, Schedule type: Karras, CFG scale: 2, Seed: 1489079570, Size: 1024x1024, Model hash: 7ac04a9474, Model: jibMixRealisticXL_v140CrystalClarity, Version: f2.0.1v1.10.1-previous-569-g6dc71b7e. Time taken: 2 min. 13.4 sec.</figcaption></figure><h3 name="a253" id="a253" class="graf graf--h3 graf-after--figure">Forge with Flux dev-fp8-model (for 1080 Ti)</h3><p name="40e5" id="40e5" class="graf graf--p graf-after--h3">After a few tests with CUDA 12.4 and related torch and xformers, I was not able to speed up Forge’s generation times to what Automatic1111 can do with same SDXL-models.</p><p name="3067" id="3067" class="graf graf--p graf-after--p">For older NVIDIA cards NF4 Flux-models was wrong choise. NVIDIA 1080 Ti should not support NF4, but as you see from previous pictures, I was able to generate pictures, but very slowly.</p><p name="0c70" id="0c70" class="graf graf--p graf-after--p">Proper version for older GPU should be <a href="https://huggingface.co/lllyasviel/flux1_dev/blob/main/flux1-dev-fp8.safetensors" data-href="https://huggingface.co/lllyasviel/flux1_dev/blob/main/flux1-dev-fp8.safetensors" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://huggingface.co/lllyasviel/flux1_dev/blob/main/flux1-dev-fp8.safetensors</a></p><p name="5f39" id="5f39" class="graf graf--p graf-after--p">Do not enable Diffusion in Low Bits settings any NF4 related settings, keep it Auto!</p><p name="5e12" id="5e12" class="graf graf--p graf-after--p">This example is generated with flux1-dev-fp8.safetensors (= no NF4) and generation time was 11 minutes.</p><figure name="7bd2" id="7bd2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NtNcv94zG9E5K4uvOktrLQ.png" data-width="896" data-height="1152" src="https://cdn-images-1.medium.com/max/800/1*NtNcv94zG9E5K4uvOktrLQ.png"><figcaption class="imageCaption">flux1-dev-fp8.safetensors with 15 steps, generation time 11 minutes</figcaption></figure><p name="5a68" id="5a68" class="graf graf--p graf-after--figure">So, based on these tests, you can run Forge with Flux models on a relatively old computer, but the speed is significantly slower compared to using Automatic1111 with SDXL models.</p><p name="127e" id="127e" class="graf graf--p graf-after--p graf--trailing">Happy testing!</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@jari.p.hiltunen" class="p-author h-card">Jari Hiltunen</a> on <a href="https://medium.com/p/2b6f543890f4"><time class="dt-published" datetime="2024-10-17T10:15:01.294Z">October 17, 2024</time></a>.</p><p><a href="https://medium.com/@jari.p.hiltunen/testing-forge-stable-diffusion-on-an-old-nvidia-1080-ti-pc-running-linux-mint-wilma-ubuntu-24-04-2b6f543890f4" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on January 26, 2025.</p></footer></article></body></html>