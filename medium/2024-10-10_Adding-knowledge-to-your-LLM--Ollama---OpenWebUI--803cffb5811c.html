<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Adding knowledge to your LLM (Ollama / OpenWebUI)</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Adding knowledge to your LLM (Ollama / OpenWebUI)</h1>
</header>
<section data-field="subtitle" class="p-summary">
How Knowledge is Added to a Language Model: A Deeper Dive
</section>
<section data-field="body" class="e-content">
<section name="1b3c" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="229c" id="229c" class="graf graf--h3 graf--leading graf--title">Adding knowledge to your LLM (Ollama / OpenWebUI)</h3><h3 name="7fb2" id="7fb2" class="graf graf--h3 graf-after--h3">How Knowledge is Added to a Language Model: A Deeper Dive</h3><p name="f4b1" id="f4b1" class="graf graf--p graf-after--h3">When it comes to fine-tuning or updating the capabilities of a large language model like LLama3.2, there are often misconceptions about how this process works. In this article, I’ll delve into the details of how knowledge is added to a model and explore the limitations and possibilities of this process.</p><h4 name="df21" id="df21" class="graf graf--h4 graf-after--p">Pre-trained Models: Background Knowledge</h4><p name="7db7" id="7db7" class="graf graf--p graf-after--h4">Most modern language models, including Llama3.2 etc, are built upon pre-trained models that have been fine-tuned on large-scale datasets. These pre-trained models serve as a foundation for LLM capabilities, providing a rich understanding of linguistic structures, semantic relationships, and contextual cues.</p><p name="946f" id="946f" class="graf graf--p graf-after--p">The pre-training process involves training the model on massive amounts of text data, which enables it to learn representations of words, phrases, and concepts that can be applied across various tasks and domains. This background knowledge is essential for adapting to new information and generating coherent responses.</p><h4 name="d95f" id="d95f" class="graf graf--h4 graf-after--p">Fine-tuning: Adaptation and Specialization</h4><p name="d253" id="d253" class="graf graf--p graf-after--h4">While the pre-trained model provides a solid foundation, fine-tuning or adaptation involves adjusting the model’s parameters to focus on specific tasks, datasets, or domains. This process requires additional training data, which can be in the form of labeled examples, text files, or even just individual sentences.</p><p name="0c90" id="0c90" class="graf graf--p graf-after--p">Fine-tuning is a crucial step in enabling language models to learn from new information and adapt to changing contexts. However, it’s essential to note that this process is still an area of ongoing research and development.</p><h4 name="b3d5" id="b3d5" class="graf graf--h4 graf-after--p">Continual Learning: The Challenge</h4><p name="2d8e" id="2d8e" class="graf graf--p graf-after--h4">One of the significant challenges in fine-tuning or updating language models is the need for continuous learning. This involves adapting to new information, tasks, and domains over time, which can be difficult due to the vast amount of data required.</p><p name="1a3a" id="1a3a" class="graf graf--p graf-after--p">Currently, most language models rely on periodic fine-tuning or updating to learn from new information. However, this approach can lead to concerns about model drift, where the updated knowledge becomes outdated before it’s fully integrated into the model.</p><h4 name="ff9a" id="ff9a" class="graf graf--h4 graf-after--p">Limitations and Possibilities</h4><p name="e3a7" id="e3a7" class="graf graf--p graf-after--h4">While there are many exciting developments in the field of few-shot learning and continual learning, there are still significant limitations to be addressed. These include:</p><p name="aa94" id="aa94" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Data availability</strong>: The amount and quality of training data required for fine-tuning or updating a language model can be daunting.<br><strong class="markup--strong markup--p-strong">Model complexity</strong>: Modern language models are often extremely complex, making it challenging to adapt them to new tasks or domains without compromising performance.<br><strong class="markup--strong markup--p-strong">Contextual understanding</strong>: While language models excel at generating responses based on patterns and associations, they still struggle with contextual understanding and common sense.</p><p name="8a3c" id="8a3c" class="graf graf--p graf-after--p">Despite these challenges, researchers and developers continue to push the boundaries of what is possible. Ongoing research in few-shot learning, continual learning, and fine-tuning holds promise for enabling language models to learn from new information and adapt to changing contexts.</p><h4 name="31be" id="31be" class="graf graf--h4 graf-after--p">Conclusion about overall learning</h4><p name="9508" id="9508" class="graf graf--p graf-after--h4">Adding knowledge to a language models involves a complex interplay between pre-training, fine-tuning, and adaptation. While the process is still evolving, researchers and developers are making significant strides in developing more sophisticated approaches to learning and updating language models.</p><h3 name="2d9b" id="2d9b" class="graf graf--h3 graf-after--p">TensorFlow: Understanding Checkpoints and Weights</h3><p name="4bcd" id="4bcd" class="graf graf--p graf-after--h3">TensorFlow is a widely-used open-source machine learning framework that has become the de facto standard for deep learning models. At its core, TensorFlow relies on the concept of variables and tensors to represent data and model parameters. In this article, I’ll explore the importance of checkpoints and weights in TensorFlow, and how they impact the performance and efficiency of your models.</p><h4 name="0211" id="0211" class="graf graf--h4 graf-after--p">Variables and Weights</h4><p name="dfb3" id="dfb3" class="graf graf--p graf-after--h4">In TensorFlow, variables are used to store model parameters, such as learned weights and biases. These variables are typically represented as tensors, which are multi-dimensional arrays that can be manipulated using tensor operations.</p><p name="d8a4" id="d8a4" class="graf graf--p graf-after--p">Weights are a specific type of variable that represent the strength or coefficient of each connection in a neural network. They play a crucial role in determining how inputs are propagated through the network and what outputs are generated.</p><h4 name="5c6c" id="5c6c" class="graf graf--h4 graf-after--p">Checkpoints</h4><p name="b4a8" id="b4a8" class="graf graf--p graf-after--h4">A checkpoint is a snapshot of the model’s weights at a particular point during training. Checkpoints are used to save the state of the model, allowing you to resume training from where you left off if something goes wrong or if you want to switch between different hyperparameters.</p><p name="7afa" id="7afa" class="graf graf--p graf-after--p">There are several reasons why checkpoints are important:</p><p name="4a13" id="4a13" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Model continuity</strong>: By saving the weights at regular intervals, you can ensure that your model continues to make progress even if you need to pause or stop training.<br><strong class="markup--strong markup--p-strong">Hyperparameter tuning</strong>: Checkpoints allow you to easily experiment with different hyperparameters and see how they impact the performance of your model.<br><strong class="markup--strong markup--p-strong">Training speed</strong>: By loading pre-trained checkpoints, you can significantly reduce the amount of time it takes to train a new model.</p><h4 name="b773" id="b773" class="graf graf--h4 graf-after--p">Types of Checkpoints</h4><p name="a4a3" id="a4a3" class="graf graf--p graf-after--h4">There are two main types of checkpoints in TensorFlow:</p><p name="daf8" id="daf8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Full checkpoint</strong>: A full checkpoint includes all the variables and weights in the model, allowing you to resume training from scratch.<br><strong class="markup--strong markup--p-strong">Partial checkpoint</strong>: A partial checkpoint includes only the most recent variables and weights, which can be useful if you’re working with limited memory or storage.</p><h4 name="0d10" id="0d10" class="graf graf--h4 graf-after--p">Weights and Their Significance</h4><p name="7c30" id="7c30" class="graf graf--p graf-after--h4">Weights play a crucial role in determining how inputs are propagated through the network and what outputs are generated. In TensorFlow, weights are typically represented as variables that can be manipulated using tensor operations.</p><p name="93f7" id="93f7" class="graf graf--p graf-after--p">The significance of weights lies in their ability to:</p><p name="0ecc" id="0ecc" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Determine the strength of connections</strong>: Weights determine the strength or coefficient of each connection in a neural network.<br><strong class="markup--strong markup--p-strong">Control the flow of information</strong>: By adjusting the values of the weights, you can control how inputs are propagated through the network and what outputs are generated.</p><h3 name="52f9" id="52f9" class="graf graf--h3 graf-after--p">How Weights are Updated</h3><p name="d005" id="d005" class="graf graf--p graf-after--h3">In TensorFlow, weights are typically updated using stochastic gradient descent (SGD) or other optimization algorithms. The goal of these algorithms is to minimize the loss function by adjusting the weights in a way that maximizes the model’s performance on the training data.</p><h4 name="7b1d" id="7b1d" class="graf graf--h4 graf-after--p">Conclusion of Tensorflow</h4><p name="cbbd" id="cbbd" class="graf graf--p graf-after--h4">In this article, I explored the importance of checkpoints and weights in TensorFlow. By understanding how to create, load, and use checkpoints, you can save your models’ progress, resume training from where you left off, and improve their performance over time.</p><p name="99a5" id="99a5" class="graf graf--p graf-after--p">Weights play a crucial role in determining how inputs are propagated through the network and what outputs are generated, making them an essential component of any machine learning model. By understanding how weights are updated using stochastic gradient descent or other optimization algorithms, you can fine-tune your models to achieve better results.</p><p name="f808" id="f808" class="graf graf--p graf-after--p">So, with limitations mentioned before, you can kind of educate your Large Language Model (LLM) using Workspaces (Työtilat) and creating topics related to the subject matter you want to teach the LLM. For example, I added Psychology information to the LLM by feeding it a 65 MB PDF file about Psychology.</p><h3 name="57ff" id="57ff" class="graf graf--h3 graf-after--p">Where OpenWebUI stores uploaded and vectorized files?</h3><p name="639b" id="639b" class="graf graf--p graf-after--h3 graf--trailing">Files are stored under installation directory. In my case I have installed open-webui under /opt, so files I upload are at path /opt/open-webui/venv/lib/python3.11/site-packages/open_webui/data/uploads$</p></div></div></section><section name="2ca6" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="2138" id="2138" class="graf graf--h3 graf--leading">About tokenizing uploaded files</h3><p name="e127" id="e127" class="graf graf--p graf-after--h3">I’m utilizing the paraphrase-multilingual:latest model and content engine for processing PDFs and other documents. While I’ve fine-tuned my RAG-model slightly, I rely on the default settings for optimal performance.</p><p name="8d59" id="8d59" class="graf graf--p graf-after--p">Once you upload your document to OpenWebUI, Ollama begins processing it. To gauge its performance, simply run the command <code class="markup--code markup--p-code">ollama ps</code> after uploading your file.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="vbnet" name="8767" id="8767" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">NAME                              ID              SIZE      PROCESSOR    <span class="hljs-keyword">UNTIL</span>               <br />paraphrase-multilingual:latest    ba13c2e06707    <span class="hljs-number">1.0</span> GB    <span class="hljs-number">100%</span> GPU     <span class="hljs-number">4</span> minutes <span class="hljs-keyword">from</span> now     <br />llama3.<span class="hljs-number">2</span>:<span class="hljs-number">3</span>b                       a80c4f17acd5    <span class="hljs-number">4.0</span> GB    <span class="hljs-number">100%</span> GPU     <span class="hljs-number">46</span> seconds <span class="hljs-keyword">from</span> now  </span></pre><p name="f631" id="f631" class="graf graf--p graf-after--pre">In this example, observe that Llama 3.2 and Paraphrase-multilingual are both leveraging GPU acceleration. In an NVIDIA environment, you can further monitor GPU utilization using the <code class="markup--code markup--p-code">nvidia-smi</code> command.</p><p name="2427" id="2427" class="graf graf--p graf-after--p">During vectorization at the OpenWebUI backend, you will notice a similar output.</p><figure name="5b41" id="5b41" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Nb6zgmoFnhlEF_wxrcQW_A.png" data-width="1088" data-height="668" src="https://cdn-images-1.medium.com/max/800/1*Nb6zgmoFnhlEF_wxrcQW_A.png"><figcaption class="imageCaption">Screen capture</figcaption></figure><p name="05aa" id="05aa" class="graf graf--p graf-after--figure graf--trailing">This screen captures that OpenWebUI is generating vectors, while the TOP command below provides further insight. Notably, it shows that <code class="markup--code markup--p-code">ollama_llama_se</code> is consuming approximately 43.9 GB of memory and utilizing 88.4% of the CPU (after uploading a 65 MB PDF file). Additionally, it&#39;s mentioned that the worker was simultaneously learning Finnish.</p></div></div></section><section name="3944" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="a324" id="a324" class="graf graf--h3 graf--leading">Am I safe if I upload secret documents to my LLM?</h3><p name="a945" id="a945" class="graf graf--p graf-after--h3">I conducted a test to verify that when using OpenWebUI in conjunction with Ollama, there are no suspicious network connections. As you can see, even while having WhatsApp open and an active connection to my NFS server, only these two applications were connected, with no other external connections present.</p><figure name="81a5" id="81a5" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="1*6K2uX9qyTtHQQDqmSHyONw.png" data-width="1300" data-height="942" src="https://cdn-images-1.medium.com/max/800/1*6K2uX9qyTtHQQDqmSHyONw.png"><figcaption class="imageCaption">Adding knowledge on top, connections below</figcaption></figure></div></div></section><section name="7fb3" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="106a" id="106a" class="graf graf--h3 graf--leading">How to Access Collections?</h3><p name="ef75" id="ef75" class="graf graf--p graf-after--h3">Once you have added knowledge and processing is complete, you can access collections by using the # character in the same way you would access Web URLs, etc.</p><p name="269c" id="269c" class="graf graf--p graf-after--p">The book I uploaded contains text and figures. For example, Figure 16.1 from ATKINSON &amp; HILGARD’S INTRODUCTION TO PSYCHOLOGY appears as follows:</p><figure name="7350" id="7350" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RsIPRZ0cgjzp36OX4FQ49A.png" data-width="617" data-height="495" src="https://cdn-images-1.medium.com/max/800/1*RsIPRZ0cgjzp36OX4FQ49A.png"><figcaption class="imageCaption">Figure from page 582 — CHAPTER 16 TREATMENT OF MENTAL HEALTH PROBLEMS</figcaption></figure><p name="c643" id="c643" class="graf graf--p graf-after--figure">Next, I wanted to test how well different models understand the provided paragraph. To do this, I posed a question using the #Psychology collection selection command: “<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Can you show me the trends in the number of psychiatric beds in Western Europe from 1978–2002?”</em></strong></p><p name="5ad8" id="5ad8" class="graf graf--p graf-after--p">Response by LLama3.2:</p><blockquote name="4bb1" id="4bb1" class="graf graf--blockquote graf-after--p">llama3.2:3b 4:01 pm</blockquote><blockquote name="2361" id="2361" class="graf graf--blockquote graf-after--blockquote">Unfortunately, the provided context information does not include a paragraph specifically titled “Trends in the numbers of psychiatric beds in western Europe 1978–2002”. However, there is a figure (Figure 16.1) and another figure (Figure 16.2) that show trends in psychiatric bed numbers for various European countries.</blockquote><blockquote name="6e47" id="6e47" class="graf graf--blockquote graf-after--blockquote">The provided context information does mention that Figure 16.1 shows “Trends in the numbers of psychiatric beds in western Europe 1978–2002” and Figure 16.2 shows “Trends in the numbers of psychiatric beds in the new EU Member States, 1988–2002”.</blockquote><blockquote name="36d8" id="36d8" class="graf graf--blockquote graf-after--blockquote">Based on this information, I can provide a general overview of the trends mentioned in these figures.</blockquote><blockquote name="b287" id="b287" class="graf graf--blockquote graf-after--blockquote">Figure 16.1 (not shown) appears to show a decrease in the number of psychiatric beds in western European countries between 1978 and 2002. However, the exact trend is not explicitly stated.</blockquote><blockquote name="496f" id="496f" class="graf graf--blockquote graf-after--blockquote">Figure 16.2, which shows “Trends in the numbers of psychiatric beds in new EU Member States, 1988–2002”, suggests that some new EU member states (e.g., Sweden) have experienced a more significant decrease in psychiatric bed numbers than others (e.g., the United Kingdom). The figure also implies that reductions in psychiatric hospitalization have occurred in many European nations.</blockquote><blockquote name="43be" id="43be" class="graf graf--blockquote graf-after--blockquote">Please note that without access to the actual figures, I can only provide a general interpretation of the trends mentioned. If you have any further questions or would like more information on this topic, please let me know!</blockquote><p name="6e0c" id="6e0c" class="graf graf--p graf-after--blockquote">Next, I switched to the Mistral model and posed the same question in a new chat session. The response was:</p><blockquote name="d5d4" id="d5d4" class="graf graf--blockquote graf-after--p">mistral:latest 4:13 pm</blockquote><blockquote name="5336" id="5336" class="graf graf--blockquote graf-after--blockquote">Figures 16.1 and 16.2 from the European health for all database, WHO Regional Office for Europe (2004), provide trends in the number of psychiatric beds in Western Europe between 1978 and 2002. Figure 16.1 shows a significant reduction in the number of patients treated in specialized psychiatric facilities across many European nations. However, reductions have been less pronounced in some countries recently admitted to the European Union. Specifically, as per Figure 16.2, new EU member states show less of a decrease in psychiatric hospitalization.</blockquote><p name="49f4" id="49f4" class="graf graf--p graf-after--blockquote">And then I tried Gemma2 with the same question in a new chat session:</p><blockquote name="4316" id="4316" class="graf graf--blockquote graf-after--p">gemma2:latest</blockquote><blockquote name="38ce" id="38ce" class="graf graf--blockquote graf-after--blockquote">Reductions in the number of patients treated in specialized psychiatric facilities have occurred in many European nations. Figure 16.1 shows trends in the numbers of psychiatric beds in western Europe from 1978 to 2002.</blockquote><p name="7c5c" id="7c5c" class="graf graf--p graf-after--blockquote">And then I tried Llava13b with the same question:</p><blockquote name="1160" id="1160" class="graf graf--blockquote graf-after--p">llava:13b 4:16 pm</blockquote><blockquote name="2083" id="2083" class="graf graf--blockquote graf-after--blockquote">Certainly! Based on the context provided, here is a paragraph that presents trends in the numbers of psychiatric beds in Western Europe between 1978 and 2002.</blockquote><blockquote name="0248" id="0248" class="graf graf--blockquote graf-after--blockquote">During this period, there were significant reductions in the number of patients treated in specialized psychiatric hospitals across many European countries. The provided figures (Figures 16.1 and 16.2) illustrate that the United Kingdom had around 450 psychiatric beds per 100,000 population in 2002, while Sweden had no psychiatric beds per 100,000 population. These figures indicate a general trend towards deinstitutionalization, with some countries experiencing more pronounced reductions than others. Despite these variations, the data demonstrates a clear movement away from institutionalized mental health care in favor of community-based treatment options during this timeframe.</blockquote><p name="1ae1" id="1ae1" class="graf graf--p graf-after--blockquote">And finally, I tried Llava-llama3:</p><figure name="cae2" id="cae2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*TQPAk44Bm7AY04l0rYRVIg.png" data-width="1077" data-height="711" src="https://cdn-images-1.medium.com/max/800/1*TQPAk44Bm7AY04l0rYRVIg.png"></figure><p name="a9df" id="a9df" class="graf graf--p graf-after--figure graf--trailing">As expected, Llava-llama3 successfully identified names in the trend figure. Well done!</p></div></div></section><section name="de25" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="8537" id="8537" class="graf graf--p graf--leading">In conclusion, I tested the ability of various models, LLama3.2, Gemma2, Llava13b, and Llava-llama3, to understand and extract insights from a given paragraph I “educated” via OpenWebUI.</p><p name="a6ea" id="a6ea" class="graf graf--p graf-after--p">While each model had its strengths and weaknesses, Llava-llama3 ultimately demonstrated the best results in identifying names in the trend figure. This testing provides valuable insight into the capabilities and limitations of these models, and can inform future research and development in natural language processing.</p><figure name="fa29" id="fa29" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="1*7W05P2bQIoSDHIsVSaMqpw.png" data-width="1024" data-height="768" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*7W05P2bQIoSDHIsVSaMqpw.png"><figcaption class="imageCaption">Happy robot!</figcaption></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@jari.p.hiltunen" class="p-author h-card">Jari Hiltunen</a> on <a href="https://medium.com/p/803cffb5811c"><time class="dt-published" datetime="2024-10-10T13:36:12.696Z">October 10, 2024</time></a>.</p><p><a href="https://medium.com/@jari.p.hiltunen/adding-knowledge-to-your-llm-ollama-openwebui-803cffb5811c" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on January 26, 2025.</p></footer></article></body></html>