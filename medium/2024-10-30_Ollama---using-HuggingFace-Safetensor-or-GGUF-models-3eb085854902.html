<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Ollama — using HuggingFace Safetensor or GGUF models</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Ollama — using HuggingFace Safetensor or GGUF models</h1>
</header>
<section data-field="subtitle" class="p-summary">
GGUF and SafeTensor File Formats: An Overview
</section>
<section data-field="body" class="e-content">
<section name="4877" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="9cdb" id="9cdb" class="graf graf--h3 graf--leading graf--title">Ollama — using HuggingFace Safetensor or GGUF models</h3><h3 name="8d9a" id="8d9a" class="graf graf--h3 graf-after--h3">GGUF and SafeTensor File Formats: An Overview</h3><p name="90e4" id="90e4" class="graf graf--p graf-after--h3">As the AI and machine learning fields evolve, so does the need for efficient and secure data formats for storing, sharing, and deploying models. Two formats, <strong class="markup--strong markup--p-strong">GGUF</strong> (Generic GPU Format) and <strong class="markup--strong markup--p-strong">SafeTensor</strong>, have become popular due to their efficiency and safety features, each serving slightly different purposes within the ML ecosystem. This article explores their specifications, use cases, and benefits and then explains how to convert them for the Ollama.</p><h4 name="3b7d" id="3b7d" class="graf graf--h4 graf-after--p">1. GGUF: Generic GPU Format</h4><p name="7215" id="7215" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">GGUF</strong> (Generic GPU Format) is a data format designed specifically for storing machine learning models that are GPU-optimized, especially for large-scale models used in deep learning and complex data processing. It provides an efficient, structured way to represent models across various GPU architectures, making them easier to deploy across different platforms.</p><p name="534d" id="534d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Key Characteristics of GGUF</strong><br>- <strong class="markup--strong markup--p-strong">GPU Optimization</strong>: GGUF is tailored for models that will be deployed on GPUs. It optimizes data arrangement for efficient use of GPU resources, resulting in faster load times and reduced computational overhead during training and inference.<br>- <strong class="markup--strong markup--p-strong">Cross-Platform Compatibility</strong>: GGUF is compatible across various GPU vendors, including NVIDIA, AMD, and Intel, ensuring that model deployment is flexible across different hardware architectures.<br>- <strong class="markup--strong markup--p-strong">Structured Model Storage</strong>: It stores data in a structured manner, with metadata that enhances the readability of stored models, allowing developers and platforms to recognize dependencies and configurations with ease.</p><p name="bd33" id="bd33" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">GGUF Use Cases</strong><br>GGUF is best suited for high-performance model storage and deployment scenarios:<br>- <strong class="markup--strong markup--p-strong">Large-Scale Deep Learning Models</strong>: For architectures like Transformer models or CNNs used in image recognition and natural language processing, where GPU acceleration is necessary.<br>- <strong class="markup--strong markup--p-strong">Cross-Platform Model Distribution</strong>: GGUF is ideal for projects requiring GPU compatibility across multiple platforms, as it allows seamless migration between systems with minimal modifications.</p><p name="83c3" id="83c3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Advantages of GGUF</strong><br>- <strong class="markup--strong markup--p-strong">Faster GPU-Based Inference</strong>: By optimizing data for GPU use, GGUF can significantly reduce inference times.<br>- <strong class="markup--strong markup--p-strong">Ease of Integration</strong>: With cross-platform support, models stored in GGUF are easier to integrate into diverse production environments.</p><h4 name="bafe" id="bafe" class="graf graf--h4 graf-after--p">2. SafeTensor: A Safer Model Storage Format</h4><p name="4fd8" id="4fd8" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">SafeTensor</strong> was developed as a response to security concerns in machine learning model deployment. Unlike traditional data formats that may be vulnerable to malicious payload injections, SafeTensor emphasizes safety and security, especially when handling untrusted or user-provided models. It is primarily designed to securely store and share tensors.</p><p name="ea6c" id="ea6c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Key Characteristics of SafeTensor</strong><br>- <strong class="markup--strong markup--p-strong">Security First</strong>: SafeTensor employs a lightweight, secure container format to prevent arbitrary code execution, which can sometimes be a risk with formats that support serialized objects like Python’s Pickle.<br>- <strong class="markup--strong markup--p-strong">Self-Describing Structure</strong>: The format includes metadata describing the tensor shapes and data types, reducing the chance of loading errors and enhancing compatibility with various ML frameworks.<br>- <strong class="markup--strong markup--p-strong">Efficient Tensor Storage</strong>: Like GGUF, SafeTensor is optimized for efficient storage, focusing on tensor data rather than complex model metadata.</p><p name="a766" id="a766" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">SafeTensor Use Cases</strong><br>SafeTensor is used in scenarios where security is paramount:<br>- <strong class="markup--strong markup--p-strong">Model Sharing in Collaborative Environments</strong>: Platforms like Hugging Face and similar repositories utilize SafeTensor for sharing models without risk of code injection.<br>- <strong class="markup--strong markup--p-strong">User-Generated Model Storage</strong>: SafeTensor is a go-to format for applications where users can upload their models or data, as it mitigates risks associated with untrusted sources.</p><p name="15ac" id="15ac" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Advantages of SafeTensor</strong><br>- <strong class="markup--strong markup--p-strong">Enhanced Security</strong>: SafeTensor mitigates the risk of code execution on model loading, making it a secure choice for public repositories and open-source model sharing.<br>- <strong class="markup--strong markup--p-strong">Compatibility with Major Frameworks</strong>: SafeTensor is supported by popular ML frameworks, including PyTorch and TensorFlow, facilitating its adoption in secure model sharing.</p><figure name="d0dd" id="d0dd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*deX4xEZ5ZjgqJScilvwYxg.png" data-width="752" data-height="281" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*deX4xEZ5ZjgqJScilvwYxg.png"><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">GGUF vs. SafeTensor: A Comparison</strong></figcaption></figure><p name="dba5" id="dba5" class="graf graf--p graf-after--figure graf--trailing">GGUF and SafeTensor serve as two complementary solutions in the machine learning ecosystem. While GGUF targets performance and scalability on GPUs, SafeTensor offers a secure alternative for sharing and storing tensor data, minimizing risks associated with untrusted sources. By selecting the right format for a specific use case, developers can ensure efficient, secure, and scalable model deployment.</p></div></div></section><section name="9c77" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="5f48" id="5f48" class="graf graf--h3 graf--leading">How to use Safetensors or GGUF as a own model in Ollama</h3><p name="9a50" id="9a50" class="graf graf--p graf-after--h3">See <a href="https://github.com/ollama/ollama/blob/main/docs/import.md" data-href="https://github.com/ollama/ollama/blob/main/docs/import.md" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Ollama’s instructions</a> about creating and importing.</p><p name="ceb2" id="ceb2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Check, that you are downloading fine-tuned models</strong>, not adapters. If you download pretrained models, they are not tuned for instruction following or chat use.</p><h4 name="df22" id="df22" class="graf graf--h4 graf-after--p">Converting Safetensors to GGUF</h4><p name="3b89" id="3b89" class="graf graf--p graf-after--h4">As an example, Finnish fine tuned models from <a href="https://huggingface.co/Finnish-NLP" data-href="https://huggingface.co/Finnish-NLP" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Finnish-NLP</a> ‘s Collections can be downloaded from <a href="https://huggingface.co/collections/Finnish-NLP/instruction-tuned-models-65c08db79fa020161be2e942" data-href="https://huggingface.co/collections/Finnish-NLP/instruction-tuned-models-65c08db79fa020161be2e942" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>. I use latest <a href="https://huggingface.co/Finnish-NLP/llama-7b-finnish-instruct-v0.2" data-href="https://huggingface.co/Finnish-NLP/llama-7b-finnish-instruct-v0.2" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Llama-7b-instruct-v0.2 for Finnish</a> in this example. <a href="https://github.com/ggerganov/llama.cpp/discussions/2948" data-href="https://github.com/ggerganov/llama.cpp/discussions/2948" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Here</a> is an excellent information how to convert Safetensors to GGUF, which Ollama can understand.</p><p name="4f83" id="4f83" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Create a virtual environment for Python first, or use PyCharm which makes it for you.</em></strong> Remember to activate source venv/bin/activate-command if you do it manually. <a href="https://docs.python.org/3/library/venv.html" data-href="https://docs.python.org/3/library/venv.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Instructions</a> how to make a Python Virtual Environment.</p><p name="be98" id="be98" class="graf graf--p graf-after--p">Procedure:</p><p name="1e15" id="1e15" class="graf graf--p graf-after--p">Step 1: choose path where to you want store files.</p><p name="3d35" id="3d35" class="graf graf--p graf-after--p">Step 2: create a python file <strong class="markup--strong markup--p-strong">download.py</strong> and then execute it with command python3 download.py:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="d766" id="d766" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> snapshot_download<br />model_id=<span class="hljs-string">&quot;Finnish-NLP/llama-7b-finnish-instruct-v0.2&quot;</span><br />snapshot_download(repo_id=model_id, local_dir=<span class="hljs-string">&quot;finnish&quot;</span>,<br />                  local_dir_use_symlinks=<span class="hljs-literal">False</span>, revision=<span class="hljs-string">&quot;main&quot;</span>)</span></pre><figure name="3cea" id="3cea" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*OtiYb6-un6wF4X9yNFVeuw.png" data-width="1895" data-height="367" src="https://cdn-images-1.medium.com/max/800/1*OtiYb6-un6wF4X9yNFVeuw.png"><figcaption class="imageCaption">Downloading model files to finnish-directory (local_dir in the download.py)</figcaption></figure><p name="eeb1" id="eeb1" class="graf graf--p graf-after--figure">Step 3: clone llama.cpp.git:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="6dd2" id="6dd2" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">git <span class="hljs-built_in">clone</span> https://github.com/ggerganov/llama.cpp.git</span></pre><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="61db" id="61db" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">Cloning into <span class="hljs-string">&#x27;llama.cpp&#x27;</span>...<br />remote: Enumerating objects: 36094, <span class="hljs-keyword">done</span>.<br />remote: Counting objects: 100% (12179/12179), <span class="hljs-keyword">done</span>.<br />remote: Compressing objects: 100% (566/566), <span class="hljs-keyword">done</span>.<br />remote: Total 36094 (delta 11916), reused 11644 (delta 11610), pack-reused 23915 (from 1)<br />Receiving objects: 100% (36094/36094), 57.67 MiB | 1.35 MiB/s, <span class="hljs-keyword">done</span>.<br />Resolving deltas: 100% (26405/26405), <span class="hljs-keyword">done</span>.</span></pre><p name="7033" id="7033" class="graf graf--p graf-after--pre">Step 4 Activate your Python virtual environment (source venv/bin/activate) and install requirements. For this you need CUDA-capable GPU:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="6429" id="6429" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">pip install -r llama.cpp/requirements.txt</span></pre><p name="1a09" id="1a09" class="graf graf--p graf-after--pre">Step 5: Convert downloaded files in (in this example finnish) directory with:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="1836" id="1836" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-built_in">cd</span> finnish (this is directory from download.py)<br /><span class="hljs-built_in">cd</span> finnish<br />python3 ../llama.cpp/convert_hf_to_gguf.py .</span></pre><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="ruby" name="505d" id="505d" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content"><span class="hljs-variable constant_">INFO</span><span class="hljs-symbol">:gguf</span>.<span class="hljs-symbol">gguf_writer:</span><span class="hljs-title class_">Writing</span> the following <span class="hljs-symbol">files:</span><br /><span class="hljs-variable constant_">INFO</span><span class="hljs-symbol">:gguf</span>.<span class="hljs-symbol">gguf_writer:</span><span class="hljs-title class_">Model</span>_Merged_V0.2_Option2-<span class="hljs-number">7</span>.0B-<span class="hljs-variable constant_">F16</span>.<span class="hljs-symbol">gguf:</span> n_tensors = <span class="hljs-number">291</span>, total_size = <span class="hljs-number">14</span>.0G<br /><span class="hljs-title class_">Writing</span>: <span class="hljs-number">100</span><span class="hljs-string">%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████|</span> <span class="hljs-number">14</span>.0G/<span class="hljs-number">14</span>.0G [<span class="hljs-number">00</span><span class="hljs-symbol">:</span><span class="hljs-number">13</span>&lt;<span class="hljs-number">00</span><span class="hljs-symbol">:</span><span class="hljs-number">00</span>, <span class="hljs-number">1</span>.00Gbyte/s]<br /><span class="hljs-variable constant_">INFO</span><span class="hljs-symbol">:hf-to-gguf</span><span class="hljs-symbol">:Model</span> successfully exported to <span class="hljs-title class_">Model</span>_Merged_V0.2_Option2-<span class="hljs-number">7</span>.0B-<span class="hljs-variable constant_">F16</span>.gguf</span></pre><p name="0c13" id="0c13" class="graf graf--p graf-after--pre">Now you have converted Safetensor-files to gguf-file, in this case name is Model_Merged_V0.2_Option2–7.0B-F16.gguf.</p><p name="a94c" id="a94c" class="graf graf--p graf-after--p">Step 6: Change merged model name to something simplier, like:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="758f" id="758f" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-built_in">mv</span> Model_Merged_V0.2_Option2–7.0B-F16.gguf llama7finnish.gguf</span></pre><p name="10c9" id="10c9" class="graf graf--p graf-after--pre">Now your GGUF-file is ready to be added to the Ollama.</p><h3 name="65ab" id="65ab" class="graf graf--h3 graf-after--p">Converting GGUF for Ollama</h3><p name="772a" id="772a" class="graf graf--p graf-after--h3">Download gguf-file or create it from Safetensors. I downlaoded Safetensors from <a href="https://huggingface.co/collections/Finnish-NLP/instruction-tuned-models-65c08db79fa020161be2e942" data-href="https://huggingface.co/collections/Finnish-NLP/instruction-tuned-models-65c08db79fa020161be2e942" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">HuggingFace</a> and converted them to gguf.</p><ol class="postList"><li name="0a51" id="0a51" class="graf graf--li graf-after--p">Go to directory (finnish in this example), where you downloaded or created the gguf-file.</li><li name="aaad" id="aaad" class="graf graf--li graf-after--li">Create model file (like mymodel.model) and change FROM pointing to downloaded or created filename:</li></ol><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="00ff" id="00ff" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content"><span class="hljs-comment"># Modelfile</span><br />FROM <span class="hljs-string">&quot;./llama7bfinnish.gguf&quot;</span><br /><br />PARAMETER stop <span class="hljs-string">&quot;&lt;|im_start|&gt;&quot;</span><br />PARAMETER stop <span class="hljs-string">&quot;&lt;|im_end|&gt;&quot;</span><br /><br />TEMPLATE <span class="hljs-string">&quot;&quot;&quot;<br />&lt;|im_start|&gt;system<br />{{ .System }}&lt;|im_end|&gt;<br />&lt;|im_start|&gt;user<br />{{ .Prompt }}&lt;|im_end|&gt;<br />&lt;|im_start|&gt;assistant<br />&quot;&quot;&quot;</span></span></pre><p name="ab80" id="ab80" class="graf graf--p graf-after--pre">3. Execute command ollama create with name you wish to use and after -f parameter name of previous model-file you created. Example:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="lua" name="aea3" id="aea3" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">ollama <span class="hljs-built_in">create</span> llama7bfinnish -f mymodel.model<br /><br /></span></pre><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="cpp" name="9eff" id="9eff" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">transferring model data <span class="hljs-number">100</span>% <br /><span class="hljs-keyword">using</span> existing layer sha256:c8423089486f69ca9dbf6ab7672734223d63496c5bfa6a8d5314008607d3bc73 <br />creating <span class="hljs-keyword">new</span> layer sha256:<span class="hljs-number">8971</span>eb8e89ce161a65232db6db5019953dbc313fc296d9e6e9d7823e395673b9 <br /><span class="hljs-keyword">using</span> existing layer sha256:f02dd72bb2423204352eabc5637b44d79d17f109fdb510a7c51455892aa2d216 <br />creating <span class="hljs-keyword">new</span> layer sha256:b271cb398de9861f55338dc49c49c4bba332787904c1b6baf53d1be3cd175aea <br />writing manifest <br />success</span></pre><p name="ba3e" id="ba3e" class="graf graf--p graf-after--pre">4. Check that model is available for Ollama:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="makefile" name="2330" id="2330" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">ollama list<br />NAME                              ID              SIZE      MODIFIED       <br /><span class="hljs-section">llama7bfinnish:latest             30838cd705bb    14 GB     42 seconds ago </span></span></pre><p name="8e58" id="8e58" class="graf graf--p graf-after--pre">5. Refresh your Open WebUI and select your chat-capable model and test it out.</p><p name="baee" id="baee" class="graf graf--p graf-after--p">This example case was so that model was not accurate, but at least you get an idea how to use HuggingFace models with Ollama!</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="vbnet" name="9e85" id="9e85" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-symbol">llama7bfinnish:</span>latest<br />content ### Task:<br />Generate <span class="hljs-number">1</span>-<span class="hljs-number">3</span> broad tags categorizing the main themes <span class="hljs-keyword">of</span> the chat history, along <span class="hljs-keyword">with</span> <span class="hljs-number">1</span>-<span class="hljs-number">3</span> more specific subtopic tags.<br /><br />### Guidelines:<br />- Start <span class="hljs-keyword">with</span> high-level domains (e.g. Science, Technology, Philosophy, Arts, Politics, Business, Health, Sports, Entertainment, Education)<br />- Consider including relevant subfields/subdomains <span class="hljs-keyword">if</span> they are strongly represented throughout the conversation<br />- <span class="hljs-keyword">If</span> content <span class="hljs-built_in">is</span> too <span class="hljs-type">short</span> (less than <span class="hljs-number">3</span> messages) <span class="hljs-built_in">or</span> too diverse, use only [<span class="hljs-string">&quot;General&quot;</span>]<br />- Use the chat<span class="hljs-comment">&#x27;s primary language; default to English if multilingual</span><br />- Prioritize accuracy over specificity</span></pre><figure name="d034" id="d034" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*Dim4yFJwwgNgoGbxou-VYQ.png" data-width="1356" data-height="237" src="https://cdn-images-1.medium.com/max/800/1*Dim4yFJwwgNgoGbxou-VYQ.png"><figcaption class="imageCaption">Funny result :)</figcaption></figure><p name="fa0e" id="fa0e" class="graf graf--p graf-after--figure graf--trailing">Enjoy!</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@jari.p.hiltunen" class="p-author h-card">Jari Hiltunen</a> on <a href="https://medium.com/p/3eb085854902"><time class="dt-published" datetime="2024-10-30T17:12:45.862Z">October 30, 2024</time></a>.</p><p><a href="https://medium.com/@jari.p.hiltunen/ollama-using-huggingface-safetensor-or-gguf-models-3eb085854902" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on January 26, 2025.</p></footer></article></body></html>