<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Using openai/whisper-large-v3-turbo locally for transcribing mp3-audiofiles to text</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Using openai/whisper-large-v3-turbo locally for transcribing mp3-audiofiles to text</h1>
</header>
<section data-field="subtitle" class="p-summary">
You can use free MP3 and MP4 to text transcribers available online. However, if you’d like to experiment with local transcription using…
</section>
<section data-field="body" class="e-content">
<section name="7d23" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="f166" id="f166" class="graf graf--h3 graf--leading graf--title">Using openai/whisper-large-v3-turbo locally for transcribing mp3-audiofiles to text</h3><p name="3159" id="3159" class="graf graf--p graf-after--h3">You can use free MP3 and MP4 to text transcribers available online. However, if you’d like to experiment with local transcription using transformer models or even integrate Whisper with Open WebUI, give this a try. This approach is especially useful in cases where privacy is paramount, such as with doctors’ notes, as all data remains local and is not transferred over the internet. With local transcription, you can maintain strict data confidentiality while benefiting from the powerful accuracy of transformer-based models.</p><p name="8063" id="8063" class="graf graf--p graf-after--p">In this example, I am using the same venv as I used for <a href="https://medium.com/@jari.p.hiltunen/ollama-using-huggingface-safetensor-or-gguf-models-3eb085854902" data-href="https://medium.com/@jari.p.hiltunen/ollama-using-huggingface-safetensor-or-gguf-models-3eb085854902" class="markup--anchor markup--p-anchor" target="_blank">Ollama — using HuggingFace Safetensor or GGUF models</a>. This means, that activate your venv by command <strong class="markup--strong markup--p-strong">source venv/bin/activate</strong> before executing pip-commands or use PyCharm created venv (feel free to create new venv if needed):</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="css" name="bfc7" id="bfc7" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">pip install <span class="hljs-attr">--upgrade</span> transformers datasets<span class="hljs-selector-attr">[audio]</span> accelerate</span></pre><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="swift" name="ecfa" id="ecfa" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content"><span class="hljs-type">Successfully</span> installed accelerate<span class="hljs-operator">-</span><span class="hljs-number">1.0</span>.<span class="hljs-number">1</span> aiohappyeyeballs<span class="hljs-operator">-</span><span class="hljs-number">2.4</span>.<span class="hljs-number">3</span> <br />aiohttp<span class="hljs-operator">-</span><span class="hljs-number">3.10</span>.<span class="hljs-number">10</span> aiosignal<span class="hljs-operator">-</span><span class="hljs-number">1.3</span>.<span class="hljs-number">1</span> attrs<span class="hljs-operator">-</span><span class="hljs-number">24.2</span>.<span class="hljs-number">0</span> audioread<span class="hljs-operator">-</span><span class="hljs-number">3.0</span>.<span class="hljs-number">1</span> cffi<span class="hljs-operator">-</span><span class="hljs-number">1.17</span>.<span class="hljs-number">1</span> <br />datasets<span class="hljs-operator">-</span><span class="hljs-number">3.0</span>.<span class="hljs-number">2</span> decorator<span class="hljs-operator">-</span><span class="hljs-number">5.1</span>.<span class="hljs-number">1</span> dill<span class="hljs-operator">-</span><span class="hljs-number">0.3</span>.<span class="hljs-number">8</span> frozenlist<span class="hljs-operator">-</span><span class="hljs-number">1.5</span>.<span class="hljs-number">0</span> fsspec<span class="hljs-operator">-</span><span class="hljs-number">2024.9</span>.<span class="hljs-number">0</span> <br />joblib<span class="hljs-operator">-</span><span class="hljs-number">1.4</span>.<span class="hljs-number">2</span> <span class="hljs-keyword">lazy</span><span class="hljs-operator">-</span>loader<span class="hljs-operator">-</span><span class="hljs-number">0.4</span> librosa<span class="hljs-operator">-</span><span class="hljs-number">0.10</span>.<span class="hljs-number">2</span>.post1 llvmlite<span class="hljs-operator">-</span><span class="hljs-number">0.43</span>.<span class="hljs-number">0</span> <br />msgpack<span class="hljs-operator">-</span><span class="hljs-number">1.1</span>.<span class="hljs-number">0</span> multidict<span class="hljs-operator">-</span><span class="hljs-number">6.1</span>.<span class="hljs-number">0</span> multiprocess<span class="hljs-operator">-</span><span class="hljs-number">0.70</span>.<span class="hljs-number">16</span> numba<span class="hljs-operator">-</span><span class="hljs-number">0.60</span>.<span class="hljs-number">0</span> <br />pandas<span class="hljs-operator">-</span><span class="hljs-number">2.2</span>.<span class="hljs-number">3</span> platformdirs<span class="hljs-operator">-</span><span class="hljs-number">4.3</span>.<span class="hljs-number">6</span> pooch<span class="hljs-operator">-</span><span class="hljs-number">1.8</span>.<span class="hljs-number">2</span> propcache<span class="hljs-operator">-</span><span class="hljs-number">0.2</span>.<span class="hljs-number">0</span> psutil<span class="hljs-operator">-</span><span class="hljs-number">6.1</span>.<span class="hljs-number">0</span> <br />pyarrow<span class="hljs-operator">-</span><span class="hljs-number">18.0</span>.<span class="hljs-number">0</span> pycparser<span class="hljs-operator">-</span><span class="hljs-number">2.22</span> python<span class="hljs-operator">-</span>dateutil<span class="hljs-operator">-</span><span class="hljs-number">2.9</span>.<span class="hljs-number">0</span>.post0 pytz<span class="hljs-operator">-</span><span class="hljs-number">2024.2</span> <br />scikit<span class="hljs-operator">-</span>learn<span class="hljs-operator">-</span><span class="hljs-number">1.5</span>.<span class="hljs-number">2</span> scipy<span class="hljs-operator">-</span><span class="hljs-number">1.14</span>.<span class="hljs-number">1</span> six<span class="hljs-operator">-</span><span class="hljs-number">1.16</span>.<span class="hljs-number">0</span> soundfile<span class="hljs-operator">-</span><span class="hljs-number">0.12</span>.<span class="hljs-number">1</span> <br />soxr<span class="hljs-operator">-</span><span class="hljs-number">0.5</span>.<span class="hljs-number">0</span>.post1 threadpoolctl<span class="hljs-operator">-</span><span class="hljs-number">3.5</span>.<span class="hljs-number">0</span> tzdata<span class="hljs-operator">-</span><span class="hljs-number">2024.2</span> xxhash<span class="hljs-operator">-</span><span class="hljs-number">3.5</span>.<span class="hljs-number">0</span> yarl<span class="hljs-operator">-</span><span class="hljs-number">1.17</span>.<span class="hljs-number">1</span></span></pre><h3 name="6812" id="6812" class="graf graf--h3 graf-after--pre">Test torch</h3><p name="90e4" id="90e4" class="graf graf--p graf-after--h3">CPU is slow for longer audio file transcriptions. Use GPU acceleration (CUDA). For the <a href="https://developer.nvidia.com/cuda-toolkit" data-href="https://developer.nvidia.com/cuda-toolkit" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">NVIDIA CUDA</a> use command:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="typescript" name="5fa0" id="5fa0" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">nvidia-smi</span></pre><p name="6d89" id="6d89" class="graf graf--p graf-after--pre">If you have CUDA installed, command shows CUDA version on top right. Then, install <a href="https://pytorch.org/get-started/locally/" data-href="https://pytorch.org/get-started/locally/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">torch</a> version related to CUDA version, example:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="css" name="207a" id="207a" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">pip3 install torch torchaudio</span></pre><p name="e641" id="e641" class="graf graf--p graf-after--pre">Then make this tiny python file and test that CUDA works:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="2a00" id="2a00" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> torch<br /><span class="hljs-built_in">print</span>(torch.cuda.is_available())  <span class="hljs-comment"># Should print True</span><br /><span class="hljs-built_in">print</span>(torch.cuda.current_device())  <span class="hljs-comment"># Should print a valid device index (e.g., 0)</span></span></pre><p name="95fb" id="95fb" class="graf graf--p graf-after--pre">Then test how whisper works by creating a python code, <a href="https://huggingface.co/openai/whisper-large-v3-turbo/blob/main/README.md" data-href="https://huggingface.co/openai/whisper-large-v3-turbo/blob/main/README.md" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">example code</a>:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="e9fb" id="e9fb" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> torch<br /><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline<br /><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br /><br /><br />device = <span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Selected device: %s&quot;</span> % device)  <span class="hljs-comment"># added this for cuda check</span><br />torch_dtype = torch.float16 <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.float32<br /><br />model_id = <span class="hljs-string">&quot;openai/whisper-large-v3-turbo&quot;</span><br /><br />model = AutoModelForSpeechSeq2Seq.from_pretrained(<br />    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=<span class="hljs-literal">True</span>, use_safetensors=<span class="hljs-literal">True</span><br />)<br />model.to(device)<br /><br />processor = AutoProcessor.from_pretrained(model_id)<br /><br />pipe = pipeline(<br />    <span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>,<br />    model=model,<br />    tokenizer=processor.tokenizer,<br />    feature_extractor=processor.feature_extractor,<br />    torch_dtype=torch_dtype,<br />    device=device,<br />    return_timestamps=<span class="hljs-literal">True</span>  <span class="hljs-comment"># added this for longer text</span><br />)<br /><br />dataset = load_dataset(<span class="hljs-string">&quot;distil-whisper/librispeech_long&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)<br />sample = dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]<br /><br />result = pipe(sample)<br /><span class="hljs-built_in">print</span>(result[<span class="hljs-string">&quot;text&quot;</span>])</span></pre><p name="f5d6" id="f5d6" class="graf graf--p graf-after--pre">Note, that you shall see selected device: cuda:0 as a first line in console.</p><p name="c089" id="c089" class="graf graf--p graf-after--p">Test result :</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="vbnet" name="6070" id="6070" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Mr. Quilter <span class="hljs-built_in">is</span> the apostle <span class="hljs-keyword">of</span> the middle classes, <span class="hljs-built_in">and</span> we are glad <span class="hljs-keyword">to</span> <br />welcome his gospel. Nor <span class="hljs-built_in">is</span> Mr. Quilter<span class="hljs-comment">&#x27;s manner less interesting than his </span><br />matter. He tells us that at this festive season <span class="hljs-keyword">of</span> the year, <span class="hljs-keyword">with</span> <br />Christmas <span class="hljs-built_in">and</span> roast beef looming before us, similes drawn <span class="hljs-keyword">from</span> eating <br /><span class="hljs-built_in">and</span> its results occur most readily <span class="hljs-keyword">to</span> the mind. He has grave doubts whether <br />Sir Frederick Layton<span class="hljs-comment">&#x27;s work is really Greek after all, and can discover </span><br /><span class="hljs-keyword">in</span> it but little <span class="hljs-keyword">of</span> rocky Ithaca. Linnell<span class="hljs-comment">&#x27;s pictures are a sort of Up Guards </span><br /><span class="hljs-built_in">and</span> Adam paintings, <span class="hljs-built_in">and</span> Mason<span class="hljs-comment">&#x27;s exquisite idles are as national as a jingo </span><br />poem. Mr. Birkett Foster<span class="hljs-comment">&#x27;s landscapes smile at one much in the same way that</span><br />Mr. Carker used <span class="hljs-keyword">to</span> flash his teeth, <span class="hljs-built_in">and</span> Mr. John Collier gives his sitter a <br />cheerful slap <span class="hljs-keyword">on</span> the back before he says, <span class="hljs-built_in">like</span> a shampooer <span class="hljs-keyword">in</span> a Turkish bath,<br /><span class="hljs-keyword">next</span> man.</span></pre><p name="63ad" id="63ad" class="graf graf--p graf-after--pre graf--trailing">Now your whisper should work. Next test with mp3-file. For that we need ffmpeg.</p></div></div></section><section name="528e" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="b191" id="b191" class="graf graf--h3 graf--leading">Transcribing mp3 or mp4 files to text</h3><p name="02df" id="02df" class="graf graf--p graf-after--h3">Check that you have ffmpeg installed:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="1168" id="1168" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">whereis ffmpeg<br />ffmpeg -version</span></pre><p name="0ebc" id="0ebc" class="graf graf--p graf-after--pre">If you do not have ffmpeg installed:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="typescript" name="615c" id="615c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">sudo apt install ffmpeg</span></pre><p name="1ab2" id="1ab2" class="graf graf--p graf-after--pre">Then install ffmpeg to your Python virtual environment venv:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="typescript" name="b012" id="b012" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">pip install ffmpeg</span></pre><p name="80b2" id="80b2" class="graf graf--p graf-after--pre">Let’s test with Finnish language transcription with testaudio.mp3 (35MB filesize) which I took from an audio book.</p><p name="0a69" id="0a69" class="graf graf--p graf-after--p">I placed filename in result = pipe(“testaudio.mp3”)</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="624a" id="624a" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> torch<br /><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline<br /><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br /><br /><br />device = <span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Selected device: %s&quot;</span> % device)<br />torch_dtype = torch.float16 <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.float32<br /><br />model_id = <span class="hljs-string">&quot;openai/whisper-large-v3-turbo&quot;</span><br /><br />model = AutoModelForSpeechSeq2Seq.from_pretrained(<br />    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=<span class="hljs-literal">True</span>, use_safetensors=<span class="hljs-literal">True</span><br />)<br />model.to(device)<br /><br />processor = AutoProcessor.from_pretrained(model_id)<br /><br />pipe = pipeline(<br />    <span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>,<br />    model=model,<br />    tokenizer=processor.tokenizer,<br />    feature_extractor=processor.feature_extractor,<br />    torch_dtype=torch_dtype,<br />    device=device,<br />    return_timestamps=<span class="hljs-literal">True</span><br />)<br /><br />dataset = load_dataset(<span class="hljs-string">&quot;distil-whisper/librispeech_long&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)<br />sample = dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]<br /><br />result = pipe(<span class="hljs-string">&quot;testaudio.mp3&quot;</span>)  <span class="hljs-comment"># this is a audiobook file, 35MB</span><br /><span class="hljs-built_in">print</span>(result[<span class="hljs-string">&quot;text&quot;</span>])</span></pre><p name="472a" id="472a" class="graf graf--p graf-after--pre">You can use multiple files in pipe as described in instructions.</p><p name="4ec8" id="4ec8" class="graf graf--p graf-after--p">During processing, you can check GPU memory usage with nvidia-smi. In this test VRAM was used 2453MB. Note, that language was detected automatically. You can also define which language you would like to use for transcription. In my case, audio book transciption result was:</p><p name="3919" id="3919" class="graf graf--p graf--startsWithDoubleQuote graf-after--p">“Helmi. Tammen äänikirja. Kirjoittanut John Steinbeck. Suomentanut Alex Mattson. Lukia Ismo Kallio. Levy 1. Kaupungissa kerrotaan tarinaa suuresta helmestä. Kuinka se löytyi ja kuinka se jälleen katosi. Kerrotaan Kinosta, Kalastajasta ja hänen vaimostaan Huaanasta sekä Kojotiitosta, heidän lapsestaan. Ja koska tuota tarinaa kerrotaan yhä uudestaan, se on juurtunut kaikkien mieleen. Ja kuten kaikissa monesti kerrotuissa tarinoissa, jotka elävät kansan sydämessä, siinäkin on vain hyvää ja huonoa. Vain mustaa ja valkoista. Hyvyyttä ja pahuutta, eikä mitään siltä väliltä. Jos tämä tarina on vertauksellinen, niin ehkä jokainen löytää sille oman selityksensä ja lukiessaan sijoittaa siihen oman elämänsä. Oli miten oli….</p><p name="3d4c" id="3d4c" class="graf graf--p graf-after--p">… removed a lot of text …</p><p name="8233" id="8233" class="graf graf--p graf-after--p graf--trailing">joka valaisi maata ja kummankin jalkoja. Miehet kääntyivät kinon risuaidassa olevasta aukosta sisään ja tulivat ovelle. Ja Kino näki, että toinen oli lääkäri ja toinen palvelija, joka oli aamulla avannut portin. Kinoon oikeaan käden rystyset kuumenivat, kun hän tunsi tulokkaat. Lääkäri sanoi, en ollut kotona, kun aamulla kävit luonani, mutta nyt heti ensimmäisen tilaisuuden tarjoutuessa tulin katsomaan lastasi. Kino seisoi oviaukossa tukkiensä ruumillaan ja hänen silmiensä takana riehui ja leimusi viha, mutta myös pelko, sillä vuosisatojen alistuminen istui syvällä hänen lihassaan. Lapsi on nyt melkein terve, hän sanoi lyhyesti. Lääkäri hymyili, mutta pienten rasvapussien reunustamat silmät eivät hymyilleet. Hän sanoi, joskus ystäväiseni skorpioonin pisto vaikuttaa perimerkillisellä tavalla. Paranneminen näyttää taatulta ja sitten aivan ilman varoitusta. Hän mutristi huuliaan ja päästi pienen puhahduksen näyttäykseen, kuinka äkisti loppu saattoi tulla. Ja hän siirteli pientä mustaa lääkärinlaukkuaan saadakseen lyhdyn säteet valaisemaan sitä, sillä hän tiesi, että kinon rotuun kuuluva rakastaa kaikkia ammattivehkeitä ja luottaa niihin. Joskus, lääkäri jatkoi luistavasti, on seurauksena näivettynyt jalka tai menetetty silmä tai kyttyräselkä. Oi, minä kyllä tunnen skorpioonin piston ystäväiseni ja osaan myös sen parantaa. Jatkuu levyllä kaksi.”</p></div></div></section><section name="9e61" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="7bb0" id="7bb0" class="graf graf--p graf--leading">So, the test was successful, and transcription was completed locally without any costs (except for the electricity used by your PC or server).<br>If you’d like, consider creating a tool for Open WebUI and integrating this feature into it.</p><p name="c3e4" id="c3e4" class="graf graf--p graf-after--p">Enjoy!</p><figure name="803b" id="803b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*lrAWt75wEgFsHqz4cIq6ZA.png" data-width="1024" data-height="1024" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*lrAWt75wEgFsHqz4cIq6ZA.png"><figcaption class="imageCaption">OT can Python too</figcaption></figure><h3 name="00a3" id="00a3" class="graf graf--h3 graf-after--figure">For reference: pip freeze</h3><p name="2d3c" id="2d3c" class="graf graf--p graf-after--h3">Sometimes it is easier to understand why something does not work by comparing setups. I had these packages installed (use command pip freeze to check yours):</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="ini" name="ef44" id="ef44" class="graf graf--pre graf-after--p graf--trailing graf--preV2"><span class="pre--content"><span class="hljs-attr">accelerate</span>==<span class="hljs-number">1.0</span>.<span class="hljs-number">1</span><br /><span class="hljs-attr">aiohappyeyeballs</span>==<span class="hljs-number">2.4</span>.<span class="hljs-number">3</span><br /><span class="hljs-attr">aiohttp</span>==<span class="hljs-number">3.10</span>.<span class="hljs-number">10</span><br /><span class="hljs-attr">aiosignal</span>==<span class="hljs-number">1.3</span>.<span class="hljs-number">1</span><br /><span class="hljs-attr">attrs</span>==<span class="hljs-number">24.2</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">audioread</span>==<span class="hljs-number">3.0</span>.<span class="hljs-number">1</span><br /><span class="hljs-attr">certifi</span>==<span class="hljs-number">2024.8</span>.<span class="hljs-number">30</span><br /><span class="hljs-attr">cffi</span>==<span class="hljs-number">1.17</span>.<span class="hljs-number">1</span><br /><span class="hljs-attr">charset-normalizer</span>==<span class="hljs-number">3.4</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">datasets</span>==<span class="hljs-number">3.0</span>.<span class="hljs-number">2</span><br /><span class="hljs-attr">decorator</span>==<span class="hljs-number">5.1</span>.<span class="hljs-number">1</span><br /><span class="hljs-attr">dill</span>==<span class="hljs-number">0.3</span>.<span class="hljs-number">8</span><br /><span class="hljs-attr">diskcache</span>==<span class="hljs-number">5.6</span>.<span class="hljs-number">3</span><br /><span class="hljs-attr">ffmpeg</span>==<span class="hljs-number">1.4</span><br /><span class="hljs-attr">filelock</span>==<span class="hljs-number">3.16</span>.<span class="hljs-number">1</span><br /><span class="hljs-attr">frozenlist</span>==<span class="hljs-number">1.5</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">fsspec</span>==<span class="hljs-number">2024.9</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">gguf</span>==<span class="hljs-number">0.10</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">huggingface-hub</span>==<span class="hljs-number">0.26</span>.<span class="hljs-number">2</span><br /><span class="hljs-attr">idna</span>==<span class="hljs-number">3.10</span><br /><span class="hljs-attr">Jinja2</span>==<span class="hljs-number">3.1</span>.<span class="hljs-number">4</span><br /><span class="hljs-attr">joblib</span>==<span class="hljs-number">1.4</span>.<span class="hljs-number">2</span><br /><span class="hljs-attr">lazy_loader</span>==<span class="hljs-number">0.4</span><br /><span class="hljs-attr">librosa</span>==<span class="hljs-number">0.10</span>.<span class="hljs-number">2</span>.post1<br /><span class="hljs-attr">llama_cpp_python</span>==<span class="hljs-number">0.3</span>.<span class="hljs-number">1</span><br /><span class="hljs-attr">llvmlite</span>==<span class="hljs-number">0.43</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">MarkupSafe</span>==<span class="hljs-number">3.0</span>.<span class="hljs-number">2</span><br /><span class="hljs-attr">mpmath</span>==<span class="hljs-number">1.3</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">msgpack</span>==<span class="hljs-number">1.1</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">multidict</span>==<span class="hljs-number">6.1</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">multiprocess</span>==<span class="hljs-number">0.70</span>.<span class="hljs-number">16</span><br /><span class="hljs-attr">networkx</span>==<span class="hljs-number">3.4</span>.<span class="hljs-number">2</span><br /><span class="hljs-attr">numba</span>==<span class="hljs-number">0.60</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">numpy</span>==<span class="hljs-number">1.26</span>.<span class="hljs-number">4</span><br /><span class="hljs-attr">nvidia-cublas-cu12</span>==<span class="hljs-number">12.4</span>.<span class="hljs-number">5.8</span><br /><span class="hljs-attr">nvidia-cuda-cupti-cu12</span>==<span class="hljs-number">12.4</span>.<span class="hljs-number">127</span><br /><span class="hljs-attr">nvidia-cuda-nvrtc-cu12</span>==<span class="hljs-number">12.4</span>.<span class="hljs-number">127</span><br /><span class="hljs-attr">nvidia-cuda-runtime-cu12</span>==<span class="hljs-number">12.4</span>.<span class="hljs-number">127</span><br /><span class="hljs-attr">nvidia-cudnn-cu12</span>==<span class="hljs-number">9.1</span>.<span class="hljs-number">0.70</span><br /><span class="hljs-attr">nvidia-cufft-cu12</span>==<span class="hljs-number">11.2</span>.<span class="hljs-number">1.3</span><br /><span class="hljs-attr">nvidia-curand-cu12</span>==<span class="hljs-number">10.3</span>.<span class="hljs-number">5.147</span><br /><span class="hljs-attr">nvidia-cusolver-cu12</span>==<span class="hljs-number">11.6</span>.<span class="hljs-number">1.9</span><br /><span class="hljs-attr">nvidia-cusparse-cu12</span>==<span class="hljs-number">12.3</span>.<span class="hljs-number">1.170</span><br /><span class="hljs-attr">nvidia-nccl-cu12</span>==<span class="hljs-number">2.21</span>.<span class="hljs-number">5</span><br /><span class="hljs-attr">nvidia-nvjitlink-cu12</span>==<span class="hljs-number">12.4</span>.<span class="hljs-number">127</span><br /><span class="hljs-attr">nvidia-nvtx-cu12</span>==<span class="hljs-number">12.4</span>.<span class="hljs-number">127</span><br /><span class="hljs-attr">packaging</span>==<span class="hljs-number">24.1</span><br /><span class="hljs-attr">pandas</span>==<span class="hljs-number">2.2</span>.<span class="hljs-number">3</span><br /><span class="hljs-attr">platformdirs</span>==<span class="hljs-number">4.3</span>.<span class="hljs-number">6</span><br /><span class="hljs-attr">pooch</span>==<span class="hljs-number">1.8</span>.<span class="hljs-number">2</span><br /><span class="hljs-attr">propcache</span>==<span class="hljs-number">0.2</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">protobuf</span>==<span class="hljs-number">4.25</span>.<span class="hljs-number">5</span><br /><span class="hljs-attr">psutil</span>==<span class="hljs-number">6.1</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">pyarrow</span>==<span class="hljs-number">18.0</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">pycparser</span>==<span class="hljs-number">2.22</span><br /><span class="hljs-attr">python-dateutil</span>==<span class="hljs-number">2.9</span>.<span class="hljs-number">0</span>.post0<br /><span class="hljs-attr">pytz</span>==<span class="hljs-number">2024.2</span><br /><span class="hljs-attr">PyYAML</span>==<span class="hljs-number">6.0</span>.<span class="hljs-number">2</span><br /><span class="hljs-attr">regex</span>==<span class="hljs-number">2024.9</span>.<span class="hljs-number">11</span><br /><span class="hljs-attr">requests</span>==<span class="hljs-number">2.32</span>.<span class="hljs-number">3</span><br /><span class="hljs-attr">safetensors</span>==<span class="hljs-number">0.4</span>.<span class="hljs-number">5</span><br /><span class="hljs-attr">scikit-learn</span>==<span class="hljs-number">1.5</span>.<span class="hljs-number">2</span><br /><span class="hljs-attr">scipy</span>==<span class="hljs-number">1.14</span>.<span class="hljs-number">1</span><br /><span class="hljs-attr">sentencepiece</span>==<span class="hljs-number">0.2</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">setuptools</span>==<span class="hljs-number">75.3</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">six</span>==<span class="hljs-number">1.16</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">soundfile</span>==<span class="hljs-number">0.12</span>.<span class="hljs-number">1</span><br /><span class="hljs-attr">soxr</span>==<span class="hljs-number">0.5</span>.<span class="hljs-number">0</span>.post1<br /><span class="hljs-attr">sympy</span>==<span class="hljs-number">1.13</span>.<span class="hljs-number">1</span><br /><span class="hljs-attr">threadpoolctl</span>==<span class="hljs-number">3.5</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">tokenizers</span>==<span class="hljs-number">0.20</span>.<span class="hljs-number">1</span><br /><span class="hljs-attr">torch</span>==<span class="hljs-number">2.5</span>.<span class="hljs-number">1</span><br /><span class="hljs-attr">torchaudio</span>==<span class="hljs-number">2.5</span>.<span class="hljs-number">1</span><br /><span class="hljs-attr">tqdm</span>==<span class="hljs-number">4.66</span>.<span class="hljs-number">6</span><br /><span class="hljs-attr">transformers</span>==<span class="hljs-number">4.46</span>.<span class="hljs-number">1</span><br /><span class="hljs-attr">triton</span>==<span class="hljs-number">3.1</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">typing_extensions</span>==<span class="hljs-number">4.12</span>.<span class="hljs-number">2</span><br /><span class="hljs-attr">tzdata</span>==<span class="hljs-number">2024.2</span><br /><span class="hljs-attr">urllib3</span>==<span class="hljs-number">2.2</span>.<span class="hljs-number">3</span><br /><span class="hljs-attr">xxhash</span>==<span class="hljs-number">3.5</span>.<span class="hljs-number">0</span><br /><span class="hljs-attr">yarl</span>==<span class="hljs-number">1.17</span>.<span class="hljs-number">1</span></span></pre></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@jari.p.hiltunen" class="p-author h-card">Jari Hiltunen</a> on <a href="https://medium.com/p/01c24cac5851"><time class="dt-published" datetime="2024-10-31T09:19:37.115Z">October 31, 2024</time></a>.</p><p><a href="https://medium.com/@jari.p.hiltunen/using-openai-whisper-large-v3-turbo-locally-for-transcibing-mp3-audiofiles-to-text-01c24cac5851" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on January 26, 2025.</p></footer></article></body></html>