<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Fun with Ollama and WebUI</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Fun with Ollama and WebUI</h1>
</header>
<section data-field="subtitle" class="p-summary">
What is Ollama?
</section>
<section data-field="body" class="e-content">
<section name="1e40" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="7424" id="7424" class="graf graf--h3 graf--leading graf--title">Fun with Ollama and WebUI</h3><h3 name="f99c" id="f99c" class="graf graf--h3 graf-after--h3"><strong class="markup--strong markup--h3-strong">What is Ollama?</strong></h3><p name="6c4f" id="6c4f" class="graf graf--p graf-after--h3">Ollama is an open-source platform designed to let users run large language models (LLMs) locally on their machines, without the need for cloud-based services. This gives users greater control over their data privacy, as no information is transmitted to third-party servers. Initially created to support models like LLaMA, Ollama has since expanded to include other models, such as Mistral and Phi-2. Users can interact with these models through a command-line interface or a WebUI for ease of use.</p><p name="4703" id="4703" class="graf graf--p graf-after--p">One of Ollama’s main advantages is the ability to experiment with powerful language models in a local environment, particularly useful for those who prioritize privacy or want to avoid the costs associated with cloud-based APIs. It is also highly customizable, enabling users to adjust model parameters and even create new models tailored to specific needs. Additionally, Ollama supports multimodal inputs (text and images) and can be integrated into custom applications via its API.</p><p name="d05f" id="d05f" class="graf graf--p graf-after--p">In short, Ollama is a versatile, privacy-focused tool for running and experimenting with LLMs locally, offering an accessible solution for developers and researchers alike (GitHub)​(MiraMuse AI)​(Isaac Chung | Isaac Chung).</p><h3 name="5a1e" id="5a1e" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Installation and Setup</strong></h3><p name="3894" id="3894" class="graf graf--p graf-after--h3">In the following section, I’ll walk through a method that works for both Ubuntu 24.04 LTS and Linux Mint Wilma versions. I’ll also address a few common issues, particularly those related to NVIDIA CUDA, Docker (which I advise against using in this context), and memory limitations.</p><p name="fe14" id="fe14" class="graf graf--p graf-after--p">If you plan to use CUDA with Open WebUI, <a href="https://medium.com/@jari.p.hiltunen/openwebui-install-with-nvidia-cuda-support-and-without-docker-b36d79f36366" data-href="https://medium.com/@jari.p.hiltunen/openwebui-install-with-nvidia-cuda-support-and-without-docker-b36d79f36366" class="markup--anchor markup--p-anchor" target="_blank">follow these instructions</a>.</p><p name="5944" id="5944" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Installing Ollama</strong></p><p name="fabd" id="fabd" class="graf graf--p graf-after--p">For detailed installation instructions, you can follow this <a href="https://medium.com/@sridevi17j/step-by-step-guide-setting-up-and-running-ollama-in-windows-macos-linux-a00f21164bf3" data-href="https://medium.com/@sridevi17j/step-by-step-guide-setting-up-and-running-ollama-in-windows-macos-linux-a00f21164bf3" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">step-by-step guide</a>, which covers Windows, macOS, and Linux setups.</p><p name="6ee6" id="6ee6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Testing with Windows:</strong></p><ol class="postList"><li name="a719" id="a719" class="graf graf--li graf-after--p">Install Ollama by following the guide linked above.</li><li name="bd7f" id="bd7f" class="graf graf--li graf-after--li">Open the Command Prompt and run the command <code class="markup--code markup--li-code">ollama run llama3.1</code>. This will initiate the download of the LLaMA 3.1 model.</li><li name="9c53" id="9c53" class="graf graf--li graf-after--li">Once the download is complete, start testing by typing your prompt.</li></ol><p name="5ee7" id="5ee7" class="graf graf--p graf-after--li">You’ll notice that while LLaMA 3.1 is functional, it may not yet exhibit peak performance when responding intelligently. Further tweaking may be required for more complex interactions.</p><p name="db1d" id="db1d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">For Ubuntu / Mint:</strong></p><p name="80e7" id="80e7" class="graf graf--p graf-after--p">If you’re using Ubuntu, Linux Mint, or a similar Linux distribution, navigate to the <code class="markup--code markup--p-code">/opt</code> directory and install Ollama using the following command: curl -fsSL <a href="https://ollama.com/install.sh" data-href="https://ollama.com/install.sh" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://ollama.com/install.sh</a> | sh</p><p name="4cd6" id="4cd6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Finding the Installation Location:</strong></p><p name="876f" id="876f" class="graf graf--p graf-after--p">After installation, you might wonder where Ollama has been installed. To check the installation path, use the command: which ollama</p><p name="c976" id="c976" class="graf graf--p graf-after--p">This will typically show <code class="markup--code markup--p-code">/usr/local/bin/ollama</code> as the location.</p><p name="7ba2" id="7ba2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Running Ollama:</strong></p><p name="fdcb" id="fdcb" class="graf graf--p graf-after--p">Once Ollama is set up and running as a service, you can start using it by running: ollama run llama3.1</p><p name="8fc1" id="8fc1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Gemma2 vs. LLaMA 3.1:</strong></p><p name="6ed2" id="6ed2" class="graf graf--p graf-after--p">If you’re running Ollama on a system with sufficient RAM, you may want to try out the Gemma2 model as well. In my tests, Gemma2 has shown better performance and understanding compared to LLaMA 3.1, handling even simple calculations more accurately.</p><figure name="ab77" id="ab77" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="1*GL5j3uTXXYf-_0xFU3lAtQ.png" data-width="654" data-height="742" src="https://cdn-images-1.medium.com/max/800/1*GL5j3uTXXYf-_0xFU3lAtQ.png"><figcaption class="imageCaption">Testing Ollama + Gemma2 in PC with plenty RAM. As you see, even simple calculations are correct now :)</figcaption></figure></div></div></section><section name="7683" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="40a1" id="40a1" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">Frontends</strong></h3><p name="e58c" id="e58c" class="graf graf--p graf-after--h3">The choice of frontend depends on your operating system and the level of installation complexity you’re comfortable with. Do you want to run WSL2 on Windows, or would you prefer a native Windows frontend?</p><h3 name="8369" id="8369" class="graf graf--h3 graf-after--p">For Windows (without WSL2 or Docker):</h3><p name="82d0" id="82d0" class="graf graf--p graf-after--h3">If you’re using Windows (or macOS) and want to avoid WSL2 and Docker, there are several native options to run Ollama models:</p><p name="bfc9" id="bfc9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">LibreChat (Windows Support)</strong><br>LibreChat is a cross-platform frontend that works with Ollama, offering a simple and clean chat interface for interacting with large language models. It doesn’t require Docker or WSL2, making it an easy solution for native Windows users.</p><p name="1ab2" id="1ab2" class="graf graf--p graf-after--p">You can download LibreChat from its GitHub repository. This frontend provides a lightweight and efficient alternative for Windows users who want to avoid complex setups.</p><p name="b0f2" id="b0f2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">NextJS Web Interface</strong><br>Another option is the Next.js-based web UI, which allows you to run Ollama models through a web interface. This solution requires Node.js to be installed but avoids Docker entirely.</p><p name="6c3f" id="6c3f" class="graf graf--p graf-after--p">Simply install Node.js, clone the repository from GitHub, and follow the setup instructions. It provides a more user-friendly web-based interface for running models natively on Windows.</p><p name="76f1" id="76f1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Custom Desktop Applications for Windows</strong><br>Although most custom desktop apps for Ollama (such as Hollama) are currently developed for macOS, the community is working on similar solutions for Windows. It’s worth keeping an eye on GitHub for emerging Windows-native applications.</p><p name="2259" id="2259" class="graf graf--p graf-after--p graf--trailing"><strong class="markup--strong markup--p-strong">Conclusion:</strong><br>While Docker and WSL2 are common for running Ollama in Linux-heavy environments, native Windows options like LibreChat and NextJS Web Interface offer a simpler alternative. These frontends avoid the overhead of setting up Docker or WSL2, providing a more Windows-friendly experience.</p></div></div></section><section name="4a0c" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="04b0" id="04b0" class="graf graf--h3 graf--leading">For Linux, Mac, or Windows with WSL2/Hyper-V:</h3><p name="8ccc" id="8ccc" class="graf graf--p graf-after--h3">For users with Linux, macOS, or Windows capable of running WSL2 or Hyper-V, there are several frontend options that make interacting with Ollama easier:</p><p name="0061" id="0061" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Open WebUI</strong><br>This is a web-based graphical interface for Ollama, offering a visual and interactive experience compared to the command-line interface. Users can chat with models, upload documents, and adjust settings, making it ideal for those who prefer a point-and-click interface over manual commands.</p><p name="e024" id="e024" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Hollama (macOS)</strong><br>Hollama is a native macOS frontend that seamlessly integrates Ollama into the macOS environment. It offers an intuitive UI for running and interacting with models, providing an optimized experience for Mac users.</p><p name="8886" id="8886" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Ollama-SwiftUI (macOS)</strong><br>Ollama-SwiftUI is a frontend framework built with SwiftUI, designed specifically for macOS applications. It enables developers to build custom macOS apps using Ollama’s models, providing flexibility for integrating LLMs into macOS workflows.</p><p name="1876" id="1876" class="graf graf--p graf-after--p graf--trailing">These options allow users to tailor their experience based on their operating system and preferred method of interaction, whether through lightweight native applications or more visual web interfaces.</p></div></div></section><section name="7d59" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="fc53" id="fc53" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">Installing OpenWebUI Without GPU Support</strong></h3><p name="d421" id="d421" class="graf graf--p graf-after--h3">I initially attempted to install OpenWebUI with GPU support but encountered an issue that proved difficult to resolve. Below is an explanation of the problem, provided by ChatGPT:</p><blockquote name="e328" id="e328" class="graf graf--blockquote graf-after--p">CUDA and Docker Issue Explanation:</blockquote><blockquote name="697f" id="697f" class="graf graf--blockquote graf-after--blockquote">The problem you experienced with CUDA and Docker likely stems from several factors related to version mismatches between the <strong class="markup--strong markup--blockquote-strong">CUDA version</strong>, the <strong class="markup--strong markup--blockquote-strong">NVIDIA driver</strong>, and Docker’s runtime.</blockquote><blockquote name="6970" id="6970" class="graf graf--blockquote graf-after--blockquote">1. CUDA Version and Driver Mismatch</blockquote><blockquote name="f8c0" id="f8c0" class="graf graf--blockquote graf-after--blockquote">Docker containers rely on the host system’s NVIDIA drivers for CUDA support. If the <strong class="markup--strong markup--blockquote-strong">CUDA version</strong> inside the Docker container differs from the <strong class="markup--strong markup--blockquote-strong">NVIDIA driver version</strong> on the host system, you might face compatibility errors.<br>For instance, if you are running <strong class="markup--strong markup--blockquote-strong">CUDA 12.4</strong> on your system, but the Docker image requires <strong class="markup--strong markup--blockquote-strong">CUDA 12.6</strong>, you’ll see errors like:<br><code class="markup--code markup--blockquote-code">cuda&gt;=12.6, please update your driver to a newer version.</code></blockquote><blockquote name="4f1c" id="4f1c" class="graf graf--blockquote graf-after--blockquote">2. NVIDIA Runtime Configuration</blockquote><blockquote name="cbd1" id="cbd1" class="graf graf--blockquote graf-after--blockquote">Docker requires the <strong class="markup--strong markup--blockquote-strong">NVIDIA runtime</strong> to access the GPU. Even if <code class="markup--code markup--blockquote-code">nvidia-docker</code> is installed, misconfigurations in the runtime setup can prevent Docker from properly forwarding GPU requests, leading to failures.</blockquote><blockquote name="23ce" id="23ce" class="graf graf--blockquote graf-after--blockquote">3. CUDA Toolkit Compatibility</blockquote><blockquote name="54fd" id="54fd" class="graf graf--blockquote graf-after--blockquote">Docker images are often tied to specific versions of the CUDA toolkit. If your host system has a different version of the CUDA toolkit than expected by the Docker image, runtime errors may occur. This is the root cause when trying to pull images for <strong class="markup--strong markup--blockquote-strong">CUDA 12.6</strong> but running them with <strong class="markup--strong markup--blockquote-strong">CUDA 12.4</strong>.</blockquote><blockquote name="59b7" id="59b7" class="graf graf--blockquote graf-after--blockquote">4. Manifest Not Found</blockquote><blockquote name="73a5" id="73a5" class="graf graf--blockquote graf-after--blockquote">Errors like <code class="markup--code markup--blockquote-code">manifest unknown</code> can occur if you&#39;re trying to pull a Docker image that doesn&#39;t exist for the specific CUDA version, operating system, or base distribution you have.</blockquote><blockquote name="b472" id="b472" class="graf graf--blockquote graf-after--blockquote">Solutions Attempted:</blockquote><blockquote name="ed39" id="ed39" class="graf graf--blockquote graf-after--blockquote"><strong class="markup--strong markup--blockquote-strong">Pulling Compatible Docker Images:</strong> I tried using Docker images that matched the installed CUDA version (e.g., 12.4), but this didn’t resolve the driver compatibility issues.</blockquote><blockquote name="bb89" id="bb89" class="graf graf--blockquote graf-after--blockquote"><strong class="markup--strong markup--blockquote-strong">Switching to Non-GPU Support:</strong> Bypassing the GPU dependencies by switching to non-GPU support allowed Docker to run smoothly, avoiding the CUDA-driver mismatch problem.</blockquote><blockquote name="718e" id="718e" class="graf graf--blockquote graf-after--blockquote">Conclusion:</blockquote><blockquote name="6472" id="6472" class="graf graf--blockquote graf-after--blockquote graf--trailing">The primary issue stemmed from a mismatch between the <strong class="markup--strong markup--blockquote-strong">host NVIDIA driver</strong> (CUDA 12.4) and the <strong class="markup--strong markup--blockquote-strong">CUDA version required by the Docker container</strong> (CUDA 12.6). Installing a version compatible with your system’s CUDA version would resolve this.</blockquote></div></div></section><section name="fe9e" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="d02b" id="d02b" class="graf graf--h3 graf--leading">Installing OpenWebUI Without GPU Support:</h3><p name="5831" id="5831" class="graf graf--p graf-after--h3">Given the issues with GPU support, let’s proceed with installing OpenWebUI without GPU dependencies. Testing shows it’s preferable to use port 8080 to avoid problems where Ollama modules don’t appear.</p><p name="99f3" id="99f3" class="graf graf--p graf-after--p">Documentation suggests the following steps for installation:</p><p name="a5b8" id="a5b8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">OpenWebUI Server Connection Error</strong><br>If you experience connection problems, it’s often because the WebUI Docker container can’t reach the Ollama server at <code class="markup--code markup--p-code">127.0.0.1:11434</code> (<code class="markup--code markup--p-code">host.docker.internal:11434</code>) inside the container. To fix this, use the <code class="markup--code markup--p-code">--network=host</code> flag in your Docker command. Additionally, ensure the port changes from <code class="markup--code markup--p-code">3000</code> to <code class="markup--code markup--p-code">8080</code> to access the web UI at <code class="markup--code markup--p-code"><a href="http://localhost:8080." data-href="http://localhost:8080." class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">http://localhost:8080</a></code><a href="http://localhost:8080." data-href="http://localhost:8080." class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">.</a></p><h4 name="85da" id="85da" class="graf graf--h4 graf-after--p">Installation Steps:</h4><ol class="postList"><li name="5071" id="5071" class="graf graf--li graf-after--h4">Ensure Docker is installed and open a terminal (command line interface).</li><li name="7488" id="7488" class="graf graf--li graf-after--li">Run the following command: docker run -d — network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=<a href="http://127.0.0.1:11434" data-href="http://127.0.0.1:11434" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">http://127.0.0.1:11434</a> — name open-webui — restart always ghcr.io/open-webui/open-webui:main</li><li name="acde" id="acde" class="graf graf--li graf-after--li">Open your web browser and go to <code class="markup--code markup--li-code">http://localhost:8080</code>. You should see the login screen.</li><li name="e24f" id="e24f" class="graf graf--li graf-after--li">Create your login credentials and be sure to remember your password. Note that users are stored locally in a SQL database, and it’s not easy to recover them if lost.</li><li name="dbe4" id="dbe4" class="graf graf--li graf-after--li">After logging in, you should see models downloaded to Ollama.</li></ol><figure name="7bbb" id="7bbb" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*N385wl796RRCj3YQaP76Dw.png" data-width="1161" data-height="522" src="https://cdn-images-1.medium.com/max/800/1*N385wl796RRCj3YQaP76Dw.png"><figcaption class="imageCaption">Selecting module</figcaption></figure><figure name="976a" id="976a" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*HmsXJzElTi8PUxP-c7M2Nw.png" data-width="1045" data-height="770" src="https://cdn-images-1.medium.com/max/800/1*HmsXJzElTi8PUxP-c7M2Nw.png"><figcaption class="imageCaption">Asking Gemma2 capabilities</figcaption></figure><p name="5b02" id="5b02" class="graf graf--p graf-after--figure">Let’s see how well Gemma2 analyze <a href="https://github.com/divergentti/Micropython-for-ESP32/blob/main/Airquality/esp32-oled-mhz19b-pms9103m-bme680/main.py" data-href="https://github.com/divergentti/Micropython-for-ESP32/blob/main/Airquality/esp32-oled-mhz19b-pms9103m-bme680/main.py" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">my micropython code</a>:</p><figure name="d6c5" id="d6c5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DNLC4o4Dct06w-RyjYDG4w.png" data-width="1034" data-height="767" src="https://cdn-images-1.medium.com/max/800/1*DNLC4o4Dct06w-RyjYDG4w.png"><figcaption class="imageCaption">Gemma2 seems to understand Micropython</figcaption></figure><p name="3463" id="3463" class="graf graf--p graf-after--figure graf--trailing">Seems to be close to truth. Device is ESP32.</p></div></div></section><section name="f13c" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="abf8" id="abf8" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">Bad Tests: Ollama and OpenWebUI on a Laptop with 8GB RAM and NVIDIA CUDA GPU — Not Going to Work!</strong></h3><p name="3396" id="3396" class="graf graf--p graf-after--h3">Using a frontend like OpenWebUI for Ollama can enhance the experience, but if you’re working with a laptop that only has 8GB of RAM, this setup is likely to fail. OpenWebUI is available on <a href="https://github.com/open-webui/open-webui" data-href="https://github.com/open-webui/open-webui" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>, but before diving in, keep in mind that this combination will probably run into performance issues unless you’re using GPU support.</p><h4 name="298d" id="298d" class="graf graf--h4 graf-after--p">Key Problems Encountered:</h4><ol class="postList"><li name="563b" id="563b" class="graf graf--li graf-after--h4"><strong class="markup--strong markup--li-strong">Low RAM (8GB):</strong><br>Running Ollama with OpenWebUI requires significantly more RAM. If you’re attempting to run this on a system with only 8GB, performance will degrade rapidly, and Docker processes may fail to allocate sufficient resources.</li><li name="57b2" id="57b2" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">GPU Support:</strong><br>Even with a CUDA-compatible GPU, the first installation attempt often fails due to OpenWebUI trying to access Ollama through Docker’s proxy (refer to the connectivity issue mentioned earlier). Although GPU support may help, it doesn’t fully mitigate the performance bottlenecks caused by limited RAM.</li></ol><h4 name="6242" id="6242" class="graf graf--h4 graf-after--li">Steps to Install Docker on Linux Mint 24.04 (Wilma):</h4><p name="4b2b" id="4b2b" class="graf graf--p graf-after--h4">If you’re determined to try despite the limitations, here’s how to get started with Docker on Linux Mint 24.04 (Wilma). This setup mirrors Ubuntu 24.04, so we’ll begin by installing Docker:</p><ol class="postList"><li name="c52c" id="c52c" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Check Your Distribution:</strong> Run the following command to confirm your distribution information: cat /etc/upstream-release/lsb-release</li></ol><p name="61c3" id="61c3" class="graf graf--p graf-after--li">Expected output:</p><p name="9a1b" id="9a1b" class="graf graf--p graf-after--p">DISTRIB_ID=Ubuntu<br>DISTRIB_RELEASE=24.04<br>DISTRIB_CODENAME=noble<br>DISTRIB_DESCRIPTION=”Ubuntu Noble Numbat”</p><p name="cd5f" id="cd5f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Install Docker:</strong> First, update your package list and install required dependencies:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="sql" name="2c65" id="2c65" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">sudo apt <span class="hljs-keyword">update</span><br />sudo apt install apt<span class="hljs-operator">-</span>transport<span class="hljs-operator">-</span>https ca<span class="hljs-operator">-</span>certificates curl software<span class="hljs-operator">-</span>properties<span class="hljs-operator">-</span>common</span></pre><p name="4312" id="4312" class="graf graf--p graf-after--pre">3. <strong class="markup--strong markup--p-strong">Add Docker’s Repository:</strong> Add Docker’s official GPG key and set up the repository:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="c807" id="c807" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu jammy stable&quot;</span> | sudo <span class="hljs-built_in">tee</span> /etc/apt/sources.list.d/docker.list &gt; /dev/null</span></pre><p name="db79" id="db79" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">4. Install Docker:</strong> Install Docker with the following command:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="lua" name="e6f3" id="e6f3" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">sudo apt install docker.<span class="hljs-built_in">io</span></span></pre><p name="a07c" id="a07c" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">5. Test the Installation:</strong> Verify Docker is installed and working by running the test image:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="typescript" name="052f" id="052f" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">sudo docker run hello-world</span></pre><figure name="2b07" id="2b07" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*yqosE2IsTKiuPVZADCuUdA.png" data-width="641" data-height="389" src="https://cdn-images-1.medium.com/max/800/1*yqosE2IsTKiuPVZADCuUdA.png"></figure><h4 name="d49d" id="d49d" class="graf graf--h4 graf-after--figure">Conclusion:</h4><p name="d477" id="d477" class="graf graf--p graf-after--h4 graf--trailing">Be cautious if you’re working with limited resources (like 8GB RAM), as performance is likely to suffer. However, this guide should help you set up Docker and get started with OpenWebUI. If you encounter issues, like connectivity problems or failures with GPU support, it may be worth considering upgrading your system or using a more powerful machine for these tests.</p></div></div></section><section name="4f71" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="36f1" id="36f1" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">Testing OpenWebUI on a PC with Ample RAM and Nvidia GPU (including TPU Cores)</strong></h3><p name="dfa9" id="dfa9" class="graf graf--p graf-after--h3">After struggling with the previous setup on a laptop, I moved on to test OpenWebUI on a more powerful machine with sufficient RAM and an Nvidia GPU with TPU cores. This PC is already running other AI-related tools like Stable Diffusion, TensorFlow, and OpenCV. Despite the improved hardware, the installation was extremely frustrating and ultimately unsuccessful.</p><h4 name="2dba" id="2dba" class="graf graf--h4 graf-after--p">Key Issues:</h4><ol class="postList"><li name="c367" id="c367" class="graf graf--li graf-after--h4"><strong class="markup--strong markup--li-strong">Ubuntu 24.04 LTS Compatibility:</strong> Ubuntu 24.04 LTS is too new for some CUDA implementations, which may not yet be fully supported. Additionally, <strong class="markup--strong markup--li-strong">Python 3.12</strong>, which comes with this release, isn’t widely supported by many machine learning frameworks, adding further complications.</li><li name="37cd" id="37cd" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Python Version Management:</strong> To avoid interfering with the system-wide Python installation, it’s critical to install Python applications within <strong class="markup--strong markup--li-strong">virtual environments (venv)</strong>. This isolates your environment, ensuring that dependencies for one project don’t conflict with others or break system-wide Python functionality.</li><li name="d73b" id="d73b" class="graf graf--li graf-after--li">Always set up a virtual environment for Python projects, especially when dealing with cutting-edge versions like Python 3.12. Here’s how to create a venv: python3 -m venv myenv &amp; source myenv/bin/activate</li></ol><p name="2252" id="2252" class="graf graf--p graf-after--li">This allows you to install packages and dependencies locally without affecting the rest of your system.</p><p name="d1f0" id="d1f0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">4. Nvidia Driver and CUDA Compatibility:</strong> Linux Mint 24.04 LTS (Wilma) ships with <strong class="markup--strong markup--p-strong">Nvidia 535 drivers</strong>, which should be compatible with most GPUs. However, it’s essential to ensure that both the <strong class="markup--strong markup--p-strong">Nvidia drivers</strong> and <strong class="markup--strong markup--p-strong">CUDA toolkit</strong> are correctly installed and compatible with your system.</p><p name="3403" id="3403" class="graf graf--p graf-after--p">To check if your Nvidia drivers and CUDA are working properly, you can run the following command: nvidia-smi</p><p name="6bae" id="6bae" class="graf graf--p graf-after--p">This command will display the driver version, GPU utilization, and whether CUDA is available.</p><figure name="e8f5" id="e8f5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Zh8IziSgXdt0fyeLZvMVaQ.png" data-width="720" data-height="350" src="https://cdn-images-1.medium.com/max/800/1*Zh8IziSgXdt0fyeLZvMVaQ.png"></figure><p name="467f" id="467f" class="graf graf--p graf-after--figure">It seems that <strong class="markup--strong markup--p-strong">CUDA 12.2</strong> is too old for the setup we’re aiming for. To resolve this, you’ll need to update your system and install the latest drivers.</p><h4 name="fe20" id="fe20" class="graf graf--h4 graf-after--p">Steps to Update and Install New NVIDIA Drivers:</h4><ol class="postList"><li name="78b2" id="78b2" class="graf graf--li graf-after--h4"><strong class="markup--strong markup--li-strong">Update Your System:</strong> Start by updating your package list and upgrading your system packages: sudo apt update &amp; sudo apt upgrade</li><li name="9221" id="9221" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Automatically Install the Latest NVIDIA Drivers:</strong> Use the following command to install the latest recommended NVIDIA drivers automatically: sudo ubuntu-drivers autoinstall</li><li name="23af" id="23af" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Reboot:</strong> After installing the drivers, reboot your system to ensure the changes take effect: sudo reboot</li><li name="477c" id="477c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Check CUDA Version:</strong> After rebooting, use <code class="markup--code markup--li-code">nvidia-smi</code> to verify that CUDA is correctly installed and should now show <strong class="markup--strong markup--li-strong">CUDA version 12.4</strong>: nvidia-smi</li></ol><figure name="2dc0" id="2dc0" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*1YMezJFENpzjxWZKUhC-AQ.png" data-width="732" data-height="352" src="https://cdn-images-1.medium.com/max/800/1*1YMezJFENpzjxWZKUhC-AQ.png"></figure><p name="5486" id="5486" class="graf graf--p graf-after--figure">By following these steps, you should now have the latest drivers and a compatible version of CUDA installed, helping to resolve previous compatibility issues. However, I soon found myself caught in what I can only describe as a frustrating <strong class="markup--strong markup--p-strong">NVIDIA-Docker loop</strong>. While <strong class="markup--strong markup--p-strong">CUDA 12.2</strong> isn’t supported by Docker, neither is <strong class="markup--strong markup--p-strong">CUDA 12.4</strong>. For <strong class="markup--strong markup--p-strong">CUDA 12.6</strong>, new NVIDIA drivers need to be manually compiled, adding another layer of complexity.</p><h4 name="e2e7" id="e2e7" class="graf graf--h4 graf-after--p">AI Insight:</h4><p name="2a31" id="2a31" class="graf graf--p graf-after--h4">As an AI-generated response pointed out:</p><blockquote name="d9f1" id="d9f1" class="graf graf--blockquote graf-after--p">The situation with CUDA 12.4 can indeed feel like a ‘limbo’ where NVIDIA’s packaging across systems and Docker hasn’t fully aligned. This isn’t necessarily a failure, but more of a gap in release strategy or support within Docker.”</blockquote><blockquote name="d1f8" id="d1f8" class="graf graf--blockquote graf-after--blockquote">This gap creates a frustrating environment where neither <strong class="markup--strong markup--blockquote-strong">CUDA 12.2</strong> nor <strong class="markup--strong markup--blockquote-strong">CUDA 12.4</strong> is fully compatible, and upgrading to CUDA 12.6 requires additional manual compilation. It’s a clear sign of the disconnect between NVIDIA’s driver releases and Docker’s evolving support for CUDA versions.</blockquote><blockquote name="08c2" id="08c2" class="graf graf--blockquote graf-after--blockquote">It seems that Docker does not support CUDA 12.2 and CUDA 12.4. It might support CUDA 12.6, but then NVIDIA do not have latest drivers for 24.04LTS to be used with CUDA 12.6.</blockquote><p name="a7db" id="a7db" class="graf graf--p graf-after--blockquote"><a href="https://leimao.github.io/blog/NVIDIA-Docker-CUDA-Compatibility/" data-href="https://leimao.github.io/blog/NVIDIA-Docker-CUDA-Compatibility/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">I am not alone.</a> NVIDIA CUDA and Docker is not good combination!</p><h4 name="8538" id="8538" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">So, DO NOT try to install Open-WebUI with GPU support with Docker! Docker just su…s!</strong></h4><p name="94b0" id="94b0" class="graf graf--p graf-after--h4">I leave these tests here for reference.</p><p name="ff7e" id="ff7e" class="graf graf--p graf-after--p">Installing (trying to install) with GPU support and Ollama support:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="perl" name="c404" id="c404" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">docker run -d -p <span class="hljs-number">3000</span>:<span class="hljs-number">8080</span> - gpus=all -v ollama:<span class="hljs-regexp">/root/</span>.ollama -v <span class="hljs-keyword">open</span>-webui:<span class="hljs-regexp">/app/</span>backend/data - name <span class="hljs-keyword">open</span>-webui - restart always ghcr.io/<span class="hljs-keyword">open</span>-webui/<span class="hljs-keyword">open</span>-webui:ollama</span></pre><figure name="8000" id="8000" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*tZxKGzpq2KyQBR2wTvKx0A.png" data-width="758" data-height="521" src="https://cdn-images-1.medium.com/max/800/1*tZxKGzpq2KyQBR2wTvKx0A.png"><figcaption class="imageCaption">Error during install</figcaption></figure><p name="9006" id="9006" class="graf graf--p graf-after--figure">This error means that NVIDIA Container Toolkit is missing.</p><div name="afa8" id="afa8" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" data-href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html"><strong class="markup--strong markup--mixtapeEmbed-strong">Installing the NVIDIA Container Toolkit - NVIDIA Container Toolkit 1.16.0 documentation</strong><br><em class="markup--em markup--mixtapeEmbed-em">curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o…</em>docs.nvidia.com</a><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="f57778282451fa7816cf6f6920072d16" data-thumbnail-img-id="0*W2Sjb5PbeL9tckuB" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*W2Sjb5PbeL9tckuB);"></a></div><p name="e820" id="e820" class="graf graf--p graf-after--mixtapeEmbed">So, let’s install the Toolkit (easiest if you copy commands from NVIDIA page above):</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="d5b1" id="d5b1" class="graf graf--pre graf-after--p graf--trailing graf--preV2"><span class="pre--content">curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg - dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \<br /> &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \<br /> sed <span class="hljs-string">&#x27;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#x27;</span> | \<br /> sudo <span class="hljs-built_in">tee</span> /etc/apt/sources.list.d/nvidia-container-toolkit.list<br />- sudo apt-get update<br />- sudo apt-get install -y nvidia-container-toolkit<br />-sudo nvidia-ctk runtime configure - runtime=docker<br />docker pull nvidia/cuda:12.6.1-cudnn-devel-ubuntu24.04<br />docker run - gpus all nvidia/cuda:12.6.1-cudnn-devel-ubuntu24.04 nvidia-smi</span></pre></div></div></section><section name="3ef6" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="d3d0" id="d3d0" class="graf graf--h3 graf--leading">Cleaning bad install</h3><p name="7f42" id="7f42" class="graf graf--p graf-after--h3">If you end up with bad install, or loose your password for administrative access to OpenWebUI, easiest way is to clean the installation.</p><ul class="postList"><li name="6d54" id="6d54" class="graf graf--li graf-after--p">First, stop the container if it is running:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="kotlin" name="df39" id="df39" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">docker stop <span class="hljs-keyword">open</span>-webui</span></pre><ul class="postList"><li name="cbb0" id="cbb0" class="graf graf--li graf-after--pre">Then remove the container:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="900a" id="900a" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">docker <span class="hljs-built_in">rm</span> open-webui</span></pre><ul class="postList"><li name="9eca" id="9eca" class="graf graf--li graf-after--pre">After removing the container, you can delete the Docker images associated with OpenWebUI:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="perl" name="fdba" id="fdba" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">docker rmi ghcr.io/<span class="hljs-keyword">open</span>-webui/<span class="hljs-keyword">open</span>-webui:latest <span class="hljs-comment"># or </span><br />docker rmi ghcr.io/<span class="hljs-keyword">open</span>-webui/<span class="hljs-keyword">open</span>-webui:cuda <span class="hljs-comment">#If you had the CUDA version installed</span></span></pre><ul class="postList"><li name="6058" id="6058" class="graf graf--li graf-after--pre">To verify the removal of the images:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="typescript" name="6d5f" id="6d5f" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">docker images</span></pre><ul class="postList"><li name="3bf8" id="3bf8" class="graf graf--li graf-after--pre"><strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">docker images </em></strong>will list all the available Docker images. Ensure there are no <strong class="markup--strong markup--li-strong">open-webui</strong> images remaining.</li><li name="b63e" id="b63e" class="graf graf--li graf-after--li">If any volumes were created for persistent storage (e.g., <code class="markup--code markup--li-code">webui.db</code> or other configuration data), you need to remove them: 1) List all Docker volumes: <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">docker volume ls and 2) </em></strong>remove any volumes associated with OpenWebUI: <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">docker volume rm </em></strong>&lt;volume_name&gt; from above listing.</li><li name="47c8" id="47c8" class="graf graf--li graf-after--li">If you’re unsure which volume is associated with OpenWebUI, you can inspect the container setup and check for associated volumes:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="kotlin" name="20f9" id="20f9" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">docker inspect <span class="hljs-keyword">open</span>-webui</span></pre><ul class="postList"><li name="8879" id="8879" class="graf graf--li graf-after--pre">If you used mounted directories (e.g., <code class="markup--code markup--li-code">/opt/open-webui-data</code> or similar paths) for persistent storage, you can delete those directories manually:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="7534" id="7534" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">sudo <span class="hljs-built_in">rm</span> -rf /opt/open-webui-data</span></pre><ul class="postList"><li name="d07d" id="d07d" class="graf graf--li graf-after--pre">To ensure no other remnants are left (including networks, containers, images, and volumes not in use), you can run a Docker system prune:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="css" name="dff6" id="dff6" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">docker system prune -<span class="hljs-selector-tag">a</span></span></pre><p name="540b" id="540b" class="graf graf--p graf-after--pre">Usefull commands:</p><ul class="postList"><li name="71fe" id="71fe" class="graf graf--li graf-after--p">Stop service:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="kotlin" name="ebd3" id="ebd3" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">docker stop <span class="hljs-keyword">open</span>-webui</span></pre><ul class="postList"><li name="132f" id="132f" class="graf graf--li graf-after--pre">Start service:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="sql" name="777b" id="777b" class="graf graf--pre graf-after--li graf--trailing graf--preV2"><span class="pre--content">docker <span class="hljs-keyword">start</span> <span class="hljs-keyword">open</span><span class="hljs-operator">-</span>webui</span></pre></div></div></section><section name="cc36" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="6bf9" id="6bf9" class="graf graf--h3 graf--leading">Cleaning Docker totally</h3><p name="d963" id="d963" class="graf graf--p graf-after--h3">If you would like to remove Docker and Ollama Docker installs completelly, you can do it with following commands:</p><ul class="postList"><li name="7b3b" id="7b3b" class="graf graf--li graf-after--p">First, stop any running Docker containers:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="javascript" name="6bf4" id="6bf4" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">docker stop $(docker ps -q)</span></pre><ul class="postList"><li name="2f84" id="2f84" class="graf graf--li graf-after--pre">Then, remove all containers:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="64be" id="64be" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">docker <span class="hljs-built_in">rm</span> $(docker ps -a -q)</span></pre><ul class="postList"><li name="da95" id="da95" class="graf graf--li graf-after--pre">Remove all Docker images from your system:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="javascript" name="43ea" id="43ea" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">docker rmi $(docker images -q)</span></pre><ul class="postList"><li name="a967" id="a967" class="graf graf--li graf-after--pre">If you have any Docker volumes, you can remove them to free up disk space:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="1aae" id="1aae" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">docker volume <span class="hljs-built_in">rm</span> $(docker volume <span class="hljs-built_in">ls</span> -q)</span></pre><ul class="postList"><li name="bf73" id="bf73" class="graf graf--li graf-after--pre">Remove any Docker networks that were created:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="1512" id="1512" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">docker network <span class="hljs-built_in">rm</span> $(docker network <span class="hljs-built_in">ls</span> -q)</span></pre><ul class="postList"><li name="3879" id="3879" class="graf graf--li graf-after--pre">Uninstall Docker and related packages:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="csharp" name="eb48" id="eb48" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">sudo apt-<span class="hljs-keyword">get</span> purge docker-ce docker-ce-cli containerd.io</span></pre><ul class="postList"><li name="67f7" id="67f7" class="graf graf--li graf-after--pre">Remove Docker’s dependencies and related files:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="ea45" id="ea45" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">sudo apt-get autoremove<br />sudo <span class="hljs-built_in">rm</span> -rf /var/lib/docker<br />sudo <span class="hljs-built_in">rm</span> -rf /etc/docker</span></pre><ul class="postList"><li name="face" id="face" class="graf graf--li graf-after--pre">Clean up additional Docker configurations:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="c097" id="c097" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">sudo <span class="hljs-built_in">rm</span> -rf /var/lib/containerd<br />sudo <span class="hljs-built_in">rm</span> /etc/systemd/system/docker.service<br />sudo <span class="hljs-built_in">rm</span> /etc/systemd/system/docker.socket<br />sudo systemctl daemon-reload</span></pre><ul class="postList"><li name="67ae" id="67ae" class="graf graf--li graf-after--pre">Check if Docker is still installed by running: Check if Docker is still installed by running:</li></ul><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="typescript" name="68e6" id="68e6" class="graf graf--pre graf-after--li graf--trailing graf--preV2"><span class="pre--content">docker - version</span></pre></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@jari.p.hiltunen" class="p-author h-card">Jari Hiltunen</a> on <a href="https://medium.com/p/a397caf3ea11"><time class="dt-published" datetime="2024-09-08T18:36:57.301Z">September 8, 2024</time></a>.</p><p><a href="https://medium.com/@jari.p.hiltunen/fun-with-ollama-and-webui-a397caf3ea11" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on January 26, 2025.</p></footer></article></body></html>